number,mergedAt,title,summary,url
2233,2025-12-20T03:15:15Z,feat: Fused RMSNorm + FP4 Quantization Kernels in CuTe-DSL,Fused RMSNorm + FP4 Quantization Kernels (CuTe-DSL) This PR adds two high-performance fused kernels implemented in CuTe-DSL for Blackwell (â€¦,https://github.com/flashinfer-ai/flashinfer/pull/2233
2125,2025-12-20T03:09:04Z,feat: support variable sequence length in decode kernel of trtllm-gen attention,Fix #1832 This PR adds two extra parameters max_q_len and cum_seq_lens_q to flashinfer.decode.trtllm_batch_decode_with_kv_cache kernel to sâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2125
2215,2025-12-19T05:45:16Z,feat: further optimize top-k and add fused top-k page construction kernels for DSA,"Follow up of #2119 , this PR implements top_k_page_table_transform and top_k_ragged_transform function required for dsa in sglang.",https://github.com/flashinfer-ai/flashinfer/pull/2215
2243,2025-12-19T05:03:32Z,feat: RMSNorm/Fused RMSNorm + FP8 Quantization kernels,"FP8 model inference requires multiple intermediate quantization kernels, which can be avoided by fusing norm and quantization kernels.",https://github.com/flashinfer-ai/flashinfer/pull/2243
2237,2025-12-18T07:57:09Z,[feat] Integrate SGLang concat_mla_k kernel into flashinfer,Integrate SGLang optimized concat_mla_k kernel Add test as well as benchmark,https://github.com/flashinfer-ai/flashinfer/pull/2237
2047,2025-12-17T20:47:31Z,Rebase FP8 SM100 Cutlass FMHA Attention to main (original PR#1238),Just does a refresh of the FP8 Attention and adds benchmarks for Deepseek FMHA sizes.,https://github.com/flashinfer-ai/flashinfer/pull/2047
2229,2025-12-17T15:43:56Z,misc: upgrade tvm-ffi dependency to 0.1.6,Fix #2226,https://github.com/flashinfer-ai/flashinfer/pull/2229
2211,2025-12-17T15:00:18Z,Move the run function definition out of BatchedGemmInterface,Move the run function definition out of BatchedGemmInterface,https://github.com/flashinfer-ai/flashinfer/pull/2211
2213,2025-12-17T08:07:49Z,feat: Cold L2 Cache Benchmarking with Rotating Buffers,Cold L2 Cache Benchmarking for CUDA Graphs Problem bench_gpu_time_with_cudagraph captures multiple kernel iterations within a graph to amorâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2213
2214,2025-12-17T05:58:29Z,misc: support checks for gemm,Continuation of !2000,https://github.com/flashinfer-ai/flashinfer/pull/2214
2035,2025-12-16T21:07:13Z,Added an initial implementation of Q and KV Cache in fp8 and to use tâ€¦,cudnn implementation for sdpa fp8 ðŸ“Œ Description Allows cudnn SDPA be called when q and kv are both fp8 Requires cudnn 9.18.0,https://github.com/flashinfer-ai/flashinfer/pull/2035
2212,2025-12-16T04:02:38Z,cicd: Add sanity test script,This PR adds a sanity test script to use towards testing more CTK versions.,https://github.com/flashinfer-ai/flashinfer/pull/2212
2102,2025-12-15T04:06:19Z,Port TRT-LLM communication kernels to flashinfer,This ports the latest MNNVL A2A communication implementation from TRT-LLM,https://github.com/flashinfer-ai/flashinfer/pull/2102
2109,2025-12-12T21:21:17Z,feat: support more head dim in RoPE kernel,"With the new changes we should be able to support arbitrary head dim using the RopeQuantizeKernel, and I have routed the BatchQKApplyRotaryâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2109
2118,2025-12-12T19:28:03Z,Refactor trtllm_mnnvl_allreduce,This PR porting all changes in TensorRT-LLM#8018 into Flashinfer.,https://github.com/flashinfer-ai/flashinfer/pull/2118
2119,2025-12-12T07:59:14Z,perf: bunch of features and optimizations for top-k (sampling + sparse attention),"This PR implements several features and optimizations for top-k: Multi-CTA optimization Followup of #2044 , this PR optimizes the top-k/topâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2119
2194,2025-12-11T20:02:23Z,Permute page table in benchmarking,This PR will permute page tables during benchmarking of different backends and ensure the correct page table gets passed to the backends.,https://github.com/flashinfer-ai/flashinfer/pull/2194
2196,2025-12-11T07:33:28Z,docs: Fix inaccurate API docstrings for attention prefill,This PR addresses the documentation issues and adds proper validation for unsupported configurations in the attention prefill wrappers raisâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2196
2189,2025-12-09T05:16:51Z,test: Skip sm90 test in test_jit_warmup.py if not on sm90,test_jit_warmup.py contains two tests test_warmpup_llama and test_warmpup_llama_sm90 where the _sm90 variant is the first one plus an additâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2189
2165,2025-12-06T10:55:20Z,Add data type check for deepseek fp4 moe,It's so important to have dtype check to ensure API callers use FlashInfer correctly.,https://github.com/flashinfer-ai/flashinfer/pull/2165
2111,2025-12-06T03:31:41Z,refactor: update fa3 codebase and fix hopper unittest [part 1],"This PR refactors the out-dated fa3 codebase, more specifically, for page_size>1, the page offset calculation is performed inside the kerneâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2111
2171,2025-12-05T10:53:58Z,Fix gemm allreduce two shot,Fix test_cute_dsl_gemm_allreduce_two_shot.py regression from nvidia-cutlass-dsl upgrade to 4.3.1 (removed helper functions) GB300 enabled fâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2171
2163,2025-12-05T10:53:16Z,refactor: Move mla code from decode.py to mla.py and add to documentation,trtllm_batch_decode_with_kv_cache_mla and xqa_batch_decode_with_kv_cache_mla currently reside in decode.py.,https://github.com/flashinfer-ai/flashinfer/pull/2163
2159,2025-12-05T10:52:17Z,feat: MxInt4 x Bf16 TRT-LLM Gen MoE support,Add the MxInt4 x BF16 TRTLLM GEN moe,https://github.com/flashinfer-ai/flashinfer/pull/2159
2175,2025-12-05T08:19:44Z,fix: compile flags for trtllm fmha_v2 ,fix: compile flags for trtllm fmha_v2,https://github.com/flashinfer-ai/flashinfer/pull/2175
2149,2025-12-04T17:56:43Z,enable sm103 moe dsl backend,enable sm103 moe dsl backend bumped dep version nvidia-cutlass-dsl>=4.3.1,https://github.com/flashinfer-ai/flashinfer/pull/2149
2172,2025-12-04T17:56:20Z,Update Docker CI tags to 20251203-1e15fed,This PR updates the Docker CI image tags to the latest version: 20251203-1e15fed Updated images: flashinfer/flashinfer-ci-cu126:20251203-1eâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2172
2157,2025-12-04T01:36:30Z,fix xqa mha_sm90.cu,This PR fixes 2 things: 1 CUDA_ARCH is not defined in host code so xqa_wrapper.cu should use another macro 2 spec_q_seq_len is not includedâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2157
2160,2025-12-03T05:01:41Z,feat: C++ side tensor validation,Originally in this PR #1652 and #2127 we added better error messaging / prevent silent failures for wrong dtype / shape of tensors.,https://github.com/flashinfer-ai/flashinfer/pull/2160
2153,2025-12-02T22:20:40Z,misc: Label APIs for Logging,"FlashInfer's API Logging feature was enabled in #2108, but only decorated a small number of APIs with @flashinfer_api.",https://github.com/flashinfer-ai/flashinfer/pull/2153
2155,2025-12-02T08:28:42Z,Bump tvm ffi version to 0.1.4,Bump tvm ffi version to 0.1.4 and use ffi::CUDADeviceGuard instead of cudaSetDevice.,https://github.com/flashinfer-ai/flashinfer/pull/2155
2142,2025-11-28T07:35:15Z,feat: TRTLLM FMHAv2 backend for ctx attention,Porting over the trtllm fmhav2 library to support prefill cases.,https://github.com/flashinfer-ai/flashinfer/pull/2142
2089,2025-11-28T02:14:35Z,ci: Reduce test time by moving compilation off-line,Download flashinfer-cubin and flashinfer-jit-cache to avoid compilation.,https://github.com/flashinfer-ai/flashinfer/pull/2089
2132,2025-11-26T00:32:21Z,feat: add seed offset args to sampler to allow cuda graph support,This PR adds optional seed/offset args to all the sampler functions to prevent calling the get_seed_and_offset function.,https://github.com/flashinfer-ai/flashinfer/pull/2132
2138,2025-11-25T19:07:11Z,feat: add trtllm-gen per-tensor sparseMla kernels.,This MR adds trtllm-gen per-tensor sparseMla kernels.,https://github.com/flashinfer-ai/flashinfer/pull/2138
2134,2025-11-25T19:05:32Z,fix(trtllm): reset negative strideBatch to 0 for ragged KV layout to â€¦,Fix TMA descriptor failures for ragged KV layouts in the TRT-LLM FMHA path.,https://github.com/flashinfer-ai/flashinfer/pull/2134
2137,2025-11-25T19:05:16Z,fix: some bugs of headDim 256 trtllm-gen fmha kernels. ,This MR updates the trtllm-gen cubins which fix several bugs of headDim 256 fmha kernels.,https://github.com/flashinfer-ai/flashinfer/pull/2137
2126,2025-11-25T01:03:11Z,fix flaky xqa test,WIP.,https://github.com/flashinfer-ai/flashinfer/pull/2126
2128,2025-11-22T16:37:16Z,fix: DeepSeek activation uninitialized data,fix: DeepSeek activation uninitialized data,https://github.com/flashinfer-ai/flashinfer/pull/2128
2105,2025-11-22T07:54:29Z,enable xqa speculative decoding,Enable xqa with speculative decoding and add mask tensor in trtllm_batch_decode_with_kv_cache.,https://github.com/flashinfer-ai/flashinfer/pull/2105
2127,2025-11-22T07:26:02Z,fix: add a check for int32 indices in sampling.py,"New function to validate that the indices type, when provided, is int32.",https://github.com/flashinfer-ai/flashinfer/pull/2127
2108,2025-11-22T07:24:14Z,feat: Enable API Logging for Better Debugging POC,tl; dr: Current PR adds a logging system for input/output tracking to aid debugging FlashInfer APIs via a @flashinfer_api decorator.,https://github.com/flashinfer-ai/flashinfer/pull/2108
1979,2025-11-21T05:27:52Z,feat: Add backend='auto' to mm_fp4 and enable autotune for backend='cudnn',Current PR: Introduces an auto backend to mm_fp4 that can be autotuned.,https://github.com/flashinfer-ai/flashinfer/pull/1979
2112,2025-11-20T07:16:13Z,hotfix: add 9.0a to README and installation doc,"9.0a was removed from installation documentation by accident, in some recent PRs.",https://github.com/flashinfer-ai/flashinfer/pull/2112
2110,2025-11-20T06:36:45Z,add tensor scale input for xqa,add tensor scale input for xqa,https://github.com/flashinfer-ai/flashinfer/pull/2110
2117,2025-11-20T06:34:29Z,update xqa license,Update xqa license based on NVIDIA/TensorRT-LLM#8807,https://github.com/flashinfer-ai/flashinfer/pull/2117
2114,2025-11-20T02:47:57Z,feature: make the LSE returned by MLA support base 2 or e  #2113,This pr adds a parameter return_lse_base_on_e to control the base of LSE returned by MLA.,https://github.com/flashinfer-ai/flashinfer/pull/2114
2099,2025-11-19T10:33:24Z,[DSV3] Optimized routing kernels dsv3,[DSV3] Optimized routing kernels dsv3,https://github.com/flashinfer-ai/flashinfer/pull/2099
2103,2025-11-19T05:10:28Z,test: Enable testing for trtllm-gen decode bs1,"In #1898, it was raised that trtllm-gen's attention kernels fail for batch size 1.",https://github.com/flashinfer-ai/flashinfer/pull/2103
2100,2025-11-19T02:17:00Z,[DSR1] Added MLA test,"Added DSR1 MLA test, and split up the trtllm_batch_decode_mla function.",https://github.com/flashinfer-ai/flashinfer/pull/2100
2084,2025-11-18T07:53:29Z,[API change] Allow using torch.Tensor for scales for trtllm-gen attention,"change bmm1_scale and bmm2_scale to Union[float, torch.Tensor].",https://github.com/flashinfer-ai/flashinfer/pull/2084
2037,2025-11-18T07:15:14Z,"feat: Add flashinfer.rope.rope_quantize_fp8_append_paged_kv_cache (fused RoPE + Q + KV cache, supports MLA/GQA/MHA) ","Add flashinfer.rope.rope_quantize_fp8_append_paged_kv_cache, which runs a fused RoPE + Quantization (16 -> 8) + append KV Cache operation kâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2037
2088,2025-11-16T06:01:17Z,refactor: update dpsk fused_moe test [1],Refactor fused_moe test.,https://github.com/flashinfer-ai/flashinfer/pull/2088
2092,2025-11-14T21:43:40Z,perf: TRT-LLM Gen finalize kernel optimization,"Small optimization for TRT-LLM Gen MoE finalize kernel TopK=8, NumExperts=128, HiddenSize=4096 BS Baseline, us Optimized, us Speed-up",https://github.com/flashinfer-ai/flashinfer/pull/2092
2079,2025-11-14T03:55:35Z,[Feature] Support batch prefill for POD Attention,Co-authored-by: @Edenzzzz ðŸ“Œ Description Fixes #1022.,https://github.com/flashinfer-ai/flashinfer/pull/2079
2028,2025-11-13T09:54:43Z,[NVIDIA] Thor & Spark Support,Thor and Spark support when wheels are generating,https://github.com/flashinfer-ai/flashinfer/pull/2028
2083,2025-11-13T04:17:10Z,test: Change incorrect inputs in test_hopper.py,Brings in some changes to test_hopper.py to pass more unit tests test_deepseek_prefill --> Raise tolerance for bf16 inputs Others: The tokeâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2083
2080,2025-11-12T14:25:58Z,chore: update requires-python in pyproject.toml,When installing I ran into this error: Ã— No solution found when resolving dependencies for split (markers: python_full_version == '3.9.*'):â€¦,https://github.com/flashinfer-ai/flashinfer/pull/2080
2081,2025-11-12T14:25:18Z,enable xqa fp8 output,"This PR enables xqa to output fp8 tensors, and also adds an o_scale in args in interfaces in decode.py.",https://github.com/flashinfer-ai/flashinfer/pull/2081
2076,2025-11-12T05:29:05Z,fix: fix test_trtllm_gen_attention when max_seq_len < page_size,fix: fix test_trtllm_gen_attention when max_seq_len < page_size,https://github.com/flashinfer-ai/flashinfer/pull/2076
2069,2025-11-12T03:40:04Z,minor: canonicalize TFLOPS calculation ,minor: canonicalize TFLOPS calculation,https://github.com/flashinfer-ai/flashinfer/pull/2069
2075,2025-11-11T15:55:13Z,unittest: improve the efficiency of xqa unittests,The implementation of xqa unittests are sub-optimal: we use lots of cpu index calculation and slicing operations.,https://github.com/flashinfer-ai/flashinfer/pull/2075
2053,2025-11-10T15:07:31Z,feat: add xqa mla backend,add xqa mla backend and corresponding unittests,https://github.com/flashinfer-ai/flashinfer/pull/2053
2064,2025-11-09T23:14:06Z,refactor: remove MetaInfoHash class,"This class is not required after @jimmyzho 's refactor work in https://github.com/flashinfer-ai/flashinfer/pull/1967/files, and the only reâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2064
2062,2025-11-09T00:34:35Z,Fix: several bugs/issues with trtllm-gen attention kernels. ,This MR fixes: unspecified cuda launch errors with 2CTA MLA kernels masking bug of SWA decode kernels.,https://github.com/flashinfer-ai/flashinfer/pull/2062
2063,2025-11-08T06:14:05Z,perf: TRT-LLM MoE Block-FP8 activation optimization,Small optimization to the activation kernel for block-FP8 MoE for large batch size.,https://github.com/flashinfer-ai/flashinfer/pull/2063
2019,2025-11-07T22:52:59Z,[DSV3] Optimized Router Gemm,This PR: adds an optimized router gemm for problem sizes such as Deep Seek-V3.,https://github.com/flashinfer-ai/flashinfer/pull/2019
2058,2025-11-07T19:50:59Z,perf: Optimize helper max/minmax function in sampling.cuh,Apply optimizations similar to #2044 to max/min functions.,https://github.com/flashinfer-ai/flashinfer/pull/2058
2029,2025-11-07T17:17:57Z,"feat: suitable_auto_backends to prune auto backends, bmm_fp8 refactor, heuristic_func intake","feat: suitable_auto_backends to prune auto backends, bmm_fp8 refactor, heuristic_func intake",https://github.com/flashinfer-ai/flashinfer/pull/2029
2055,2025-11-07T07:07:02Z,misc: Add XQA decode to microbenchmark for sm90 and sm120,"In #2001 , XQA decode kernels became available through trtllm_batch_decode_with_kv_cache on SM90 and SM120.",https://github.com/flashinfer-ai/flashinfer/pull/2055
2044,2025-11-07T00:58:30Z,perf: improve sampling/mask/softmax performance (part 1/2),"This is the first part of the performance improvement PR for sampling/mask/softmax operator, in this PR, we defer the cross thread reductioâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/2044
2049,2025-11-06T21:33:34Z,[BUG] Fix trtllm-gen fp4 moe renormalize routing,Temporarily disable routingIndicesBlockKernel as it's not compatible with the current packing format (topk-id and expert weights are packedâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/2049
1984,2025-11-06T07:29:17Z,chore: Update CODEOWNERS,Summary This PR updates the CODEOWNERS file based on git commit history analysis from the last 180 days.,https://github.com/flashinfer-ai/flashinfer/pull/1984
1955,2025-11-06T06:06:31Z,Update trtllm-gen fused moe routing kernel and add more kernels,"co-work with @IwakuraRein update the trtllm-gen fused moe headers add new kernels for trtllm-gen fused moe for NvFp4, add tile 256 for MxFpâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1955
2033,2025-11-05T06:26:02Z,use scalar for kv_scale in xqa,use scalar for kv_scale in xqa,https://github.com/flashinfer-ai/flashinfer/pull/2033
2015,2025-11-05T06:08:19Z,Support cc common check decorator for empty backends,Support cc common check decorator for empty backends,https://github.com/flashinfer-ai/flashinfer/pull/2015
2038,2025-11-04T22:48:01Z,test: Mark test_fp8_prefill.py as xfail on SM90,"test_fp8_prefill.py is currently failing on SM90, but consumes too much time to run/fail, causing unit-tests to time out.",https://github.com/flashinfer-ai/flashinfer/pull/2038
2018,2025-11-02T06:31:36Z,test: Enable xfailed trtllm decode long seqlen tests and update microbenchmark,tests/attention/test_trtllm_gen_attention.py was failing and therefore marked xfail.,https://github.com/flashinfer-ai/flashinfer/pull/2018
2001,2025-11-02T01:06:42Z,feat: add xqa backend and completes NHD/HND coverage for trtllm-gen/xqa backend,"Expose xqa backend to trtllm attention interface, and improve layout coverage of trtllm-gen and xqa backends.",https://github.com/flashinfer-ai/flashinfer/pull/2001
2013,2025-10-31T06:48:56Z,More realistic bench for POD Attn,"Use real head sizes, seq lens and add comparison with sequential prefill + decode.",https://github.com/flashinfer-ai/flashinfer/pull/2013
2002,2025-10-29T21:45:00Z,Fix trtllm-gen attention illegal memory access,This PR fixes illegal memory access of trtllm-gen attention kernels.,https://github.com/flashinfer-ai/flashinfer/pull/2002
1980,2025-10-29T06:47:25Z,feat: autotune tile_tokens_dim in trtllm-gen MOE,Update the autotune logic in trtllm-gen moe.,https://github.com/flashinfer-ai/flashinfer/pull/1980
1999,2025-10-29T06:10:26Z,unittest: Add head dim 256 test cases and mark as xfail,Adding unit test for head_dim=256 cases for trtllm-gen decode and marking them as xfail.,https://github.com/flashinfer-ai/flashinfer/pull/1999
1973,2025-10-29T00:36:49Z,Feature: Add support for L40 FusedMoE in cutlass path,"Fixed a few compilation issues for L40, and removed 1 gemm tactic for sm == 89 that crashes due to: Assertion failed: GPU lacks the sharedâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1973
1994,2025-10-28T23:48:03Z,minor fix for xqa,1 change xqa_mla comments to be consistent with mla instead of mha.,https://github.com/flashinfer-ai/flashinfer/pull/1994
1991,2025-10-28T21:56:14Z,Added workspace check and reflected this in test,"This PR attempts to fix #1986 (to be confirmed by requester) The issue is that num_tokens was larger than MAX_TOKEN_NUM, which results in aâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1991
1998,2025-10-28T21:20:44Z,unittest: Add SM arch checks to skip unsupported tests on Hopper,"A number of unit tests fail on Hopper because they either do not have a support-check or fail based on ""what is not supported"" while missinâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1998
1952,2025-10-28T04:44:39Z,unittest: fix failed unittest on hopper,Some invalid configuration are generated in JIT warmup (mixed precision) function gen_prefill_attention_modules.,https://github.com/flashinfer-ai/flashinfer/pull/1952
1769,2025-10-27T22:56:29Z,feat: add xqa fp8 mha and fp8 kv cache,Add xqa fp8 mha and fp8 kv cache.,https://github.com/flashinfer-ai/flashinfer/pull/1769
1982,2025-10-26T17:22:39Z,fix: correct PDL parameter handling in RopeQuantize kernel,1.,https://github.com/flashinfer-ai/flashinfer/pull/1982
1969,2025-10-26T06:26:37Z,feat: enable deepgemm jit for fp8 block-scale on SM90,"Enable JIT compile for the FP8 DeepGEMM kernels, NVRTC is currently disabled it uses NVCC by default.",https://github.com/flashinfer-ai/flashinfer/pull/1969
1978,2025-10-24T23:30:44Z,fix: Skipping attention sink Blackwell test outside of Blackwell,test_attention_sink_blackwell.py checks flashinfer.prefill.trtllm_batch_context_with_kv_cache and flashinfer.decode.trtllm_batch_decode_witâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1978
1976,2025-10-24T20:03:08Z,fix: Make attention microbenchmark correctly use page table,Current microbenchmark code does not provides instantiated block_tables to all backends.,https://github.com/flashinfer-ai/flashinfer/pull/1976
1960,2025-10-24T11:14:19Z,Bump tvm ffi to stable version 0.1.0,This PR bumps the tvm-ffi to stable version 0.1.0 and update the flashinfer code base.,https://github.com/flashinfer-ai/flashinfer/pull/1960
1950,2025-10-23T16:04:43Z,unittest: fix test_artifacts.py,"This PR fixes the test suite broken by #1761 , which introduced checksum validation for downloaded artifacts.",https://github.com/flashinfer-ai/flashinfer/pull/1950
1967,2025-10-23T06:59:16Z,misc: Update artifacts docstring and MetaInfoHash,"Amendment to PR 1761, appending docstring to two artifactory path classes and deprecating need to update MetaInfoHash by directly accessingâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1967
1831,2025-10-22T16:06:45Z,Update the routing for TRTLLMGEN to support kimi k2 and qwen,Update the routing code to align with the implementation in TRTLLM and add support for KIMI K2 and Qwen Also revised the unit test based onâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1831
1912,2025-10-21T03:23:03Z,fix: Fix trtllm-gen prefill IMA when batch_size==1,Current PR fixes the test and benchmark codes IMAs when running trtllm-gen paged & ragged prefill with batch size 1 -- the issue was descriâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1912
1948,2025-10-20T03:32:58Z,"Fix #1641: Use `/usr/local/cuda` as default `CUDA_HOME` if possible, like `torch.utils.cpp_extension.CUDA_HOME`","Fix #1641: Use /usr/local/cuda as default CUDA_HOME if possible, like torch.utils.cpp_extension.CUDA_HOME ðŸ“Œ Description Before 1641, torch.â€¦",https://github.com/flashinfer-ai/flashinfer/pull/1948
1930,2025-10-18T07:49:39Z,fix get max_q_len in page prefill plan,the plan method in BatchPrefillWithPagedKVCacheWrapper will compute max query len by using qo_indptr_host.,https://github.com/flashinfer-ai/flashinfer/pull/1930
1942,2025-10-18T03:07:32Z,Add realistic bench for persistent kernel ,Previous discussion with @yzh119 included wrong results due to not skipping the module load overhead in plan().,https://github.com/flashinfer-ai/flashinfer/pull/1942
1924,2025-10-18T02:15:26Z,MLA RoPE + quantization fused kernel: shape generalization for MHA / GQA,Generalize the existing MLA RoPE+Q fused kernels to support GQA/MHA problem shapes.,https://github.com/flashinfer-ai/flashinfer/pull/1924
1761,2025-10-14T00:25:58Z,misc: checksum check when downloading artifacts,"checks the sha256 hash when downloading cubins from artifactory, using the generated checksum.txt in each cubin directory.",https://github.com/flashinfer-ai/flashinfer/pull/1761
1829,2025-10-11T06:08:01Z,feat: trtrllm-gen global scaled FP8 GEMMs,"In low latency context, it is not uncommon to encounter memory bandwidth bound GEMMs with a tiny leading dimension M.",https://github.com/flashinfer-ai/flashinfer/pull/1829
1897,2025-10-09T20:51:14Z,"tests: Add batch size 1 cases to test_trtllm_gen_attention.py that fail, marked xfail",Trtllm-gen's attention kernels have been discovered to fail tests when batch size is 1.,https://github.com/flashinfer-ai/flashinfer/pull/1897
1891,2025-10-09T06:08:45Z,misc: Various Updates to Attention Microbenchmark Suite,Current PR brings a host of updates to the the attention microbenchmark suites in flashinfer_benchmark.py testBatchPrefillWithPagedKVCacheWâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1891
1878,2025-10-07T00:53:26Z,Tune kernel compilation parameters for https://github.com/flashinfer-ai/flashinfer/pull/1850 ,A follow up to #1850 to adjust pipeline stage / tile size values for perf improvement (benchmarking results below).,https://github.com/flashinfer-ai/flashinfer/pull/1878
1865,2025-10-05T20:47:12Z,Bugfix: fix o_strides in persistent kernel ,It will be buggy when q is non-contiguous (from torch.split),https://github.com/flashinfer-ai/flashinfer/pull/1865
1850,2025-10-03T23:14:17Z,Add head_dim=64 for blackwell cutlass fmha implementation,This PR adds support for head_dim=64 for blackwell cutlass fmha.,https://github.com/flashinfer-ai/flashinfer/pull/1850
1843,2025-10-03T23:12:18Z,feat: add warp-level persistent qk norm,"Recent models are using QK normalization right before RoPE and core self-attention (e.g., Qwen-3, Wan).",https://github.com/flashinfer-ai/flashinfer/pull/1843
1825,2025-10-03T21:04:17Z,jit: add `-lcuda` to default ldflags,This PR add -lcuda to default ldflags instead of making it optional.,https://github.com/flashinfer-ai/flashinfer/pull/1825
1851,2025-10-03T09:00:43Z,unittest: remove debug-print jit examples from unittest,"The debug print statements in test_jit_examples unittests clutter the CI output, making it difficult to identify useful information.",https://github.com/flashinfer-ai/flashinfer/pull/1851
1847,2025-10-03T01:47:27Z,Run tests individually,Tests now run individually instead as opposed to with entire directry This helps avoid IMAs to propagate to other tests Also moved unlistedâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1847
1842,2025-10-02T02:23:23Z,bugfix: Change module path in test_pod_kernels.py,The series of code refactgor for tvm-ffi (e.g.,https://github.com/flashinfer-ai/flashinfer/pull/1842
1836,2025-10-02T01:05:32Z,jit: add `get_object_paths` to JitSpec,"This commit reverts changes in #1802, and adds a new method get_object_paths to JITSpec, which returns the paths of all compiled object filâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1836
1826,2025-10-01T07:15:45Z,Bugfix: Fix data hazard in persistent reduce,Same as in #1661,https://github.com/flashinfer-ai/flashinfer/pull/1826
1794,2025-09-30T17:34:46Z,chore: improved URL handling for CUBIN/artifacts downloads,Encountered some issues when adding some cubin's.,https://github.com/flashinfer-ai/flashinfer/pull/1794
1778,2025-09-29T21:58:19Z,refactor: Test reorganization phase 2,"This is the second phase of the test reorganization: Rather than using test lists, tests are now organized into directories, allowing to usâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1778
1802,2025-09-29T18:14:28Z,fix: missing header include in decode kernel jit binding,"This commit adds the missing include of the header decode.cuh in the JIT binding csrc/batch_decode.cu, without this fix, there will be linkâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1802
1795,2025-09-29T07:00:37Z,refactor: cleanup codebase after tvm-ffi refactor,"The codegen logic for pytorch and tvm should unify after #1641 , and this PR cleans up the related codegen functions in tvm_bindings.",https://github.com/flashinfer-ai/flashinfer/pull/1795
1652,2025-09-26T21:09:15Z,fix: add _check_tensor_params to check correct sampling parameters and dtype validation in decode.py,This adds the _check_tensor_params function in sampling.py that is used in the different sampling functions to check that the parameters maâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1652
1771,2025-09-25T22:24:24Z,Waive / disable test_mla_decode_kernel.py::test_mla_decode_kernel for not sm80 ,"test_mla_decode_kernel.py::test_mla_decode_kernel uses use_tensor_cores=false, which calls the internal sm80 MLA decode kernel, which doesnâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1771
1758,2025-09-25T05:06:46Z,"Fix sink attention accuracy regression, add sink test and cleanup.",Update trtllm-gen cubin to fix accuracy regression about sink.,https://github.com/flashinfer-ai/flashinfer/pull/1758
1755,2025-09-24T07:28:48Z,"Fix tests/test_trtllm_gen_attention.py::test_trtllm_batch_prefill, ::test_trtllm_batch_decode mismatch error",Fix failing tests intests/test_trtllm_gen_attention.py::test_trtllm_batch_prefill and tests/test_trtllm_gen_attention.py::test_trtllm_batchâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1755
1757,2025-09-24T06:00:05Z,fix: should pass global_override_indptr_cpu in fast_decode_plan param list,fix #1745,https://github.com/flashinfer-ai/flashinfer/pull/1757
1760,2025-09-24T03:05:58Z,test: minor update on trtllm-gen attn speculative-decoding test,test: minor update on trtllm-gen attn speculative-decoding test,https://github.com/flashinfer-ai/flashinfer/pull/1760
1752,2025-09-23T01:50:33Z,tests: xfail attention sink UT for sliding window + non causal case,"The unittests for attention sink fails after #1661 under sliding window + non causal setting (not usual but we should fix in the future), tâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1752
1746,2025-09-22T23:50:43Z,ci: complete the list of modules in aot.py,"There are still some modules not captured by aot.py, this PR completes them.",https://github.com/flashinfer-ai/flashinfer/pull/1746
1750,2025-09-22T08:16:45Z,hotfix: slightly bump up `atol` to `3e-3` to pass `test_cudnn_prefill` on B40,Slightly increase atol from 2e-3 to 3e-3 to pass the unit test.,https://github.com/flashinfer-ai/flashinfer/pull/1750
1745,2025-09-22T06:38:14Z,feat: port fast_decode_plan from sgl,feat: port fast_decode_plan from sgl,https://github.com/flashinfer-ai/flashinfer/pull/1745
1747,2025-09-21T22:54:49Z,doc: Super tiny fix doc math,doc: Super tiny fix doc math,https://github.com/flashinfer-ai/flashinfer/pull/1747
1736,2025-09-20T04:02:09Z,Test refactoring and fixes,Unit test fixes: Refactored test_mla_decode_kernel to run from pytest Added skip to test_mnnvl_custom_comm when world size is too large Addâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1736
1731,2025-09-19T18:42:46Z,Fix missing namespace qualifier,"This fixing the build of the trtllm fmhaKernel code, introduced in #1685",https://github.com/flashinfer-ai/flashinfer/pull/1731
1685,2025-09-19T09:13:55Z,perf: Port the separate reduce kernel mode from trtllm.,This also updated the kernels which fixed mha perf regression and fp8 sink attention accuracy issue.,https://github.com/flashinfer-ai/flashinfer/pull/1685
1715,2025-09-18T23:45:50Z,test: skip unsupported (non-SM90) test cases for xqa,skip unsupported (non-SM90) test cases for xqa.,https://github.com/flashinfer-ai/flashinfer/pull/1715
1710,2025-09-18T19:11:35Z,test: skip the unsupported test cases for sm120/121,test: skip the unsupported test cases for sm120/121,https://github.com/flashinfer-ai/flashinfer/pull/1710
1707,2025-09-17T23:25:20Z,bugfix: increase workspace to make trtllm gen attention unit test pass,Increase the workspace size from 128 to 256 to make sure tests/test_trtllm_gen_attention.py pass,https://github.com/flashinfer-ai/flashinfer/pull/1707
1700,2025-09-17T16:36:27Z,ci: fix prefill attention unittests,"xfail the prefill + cudagraph UT, until we fully fix the workspace overflow issues.",https://github.com/flashinfer-ai/flashinfer/pull/1700
1667,2025-09-16T14:28:58Z,Refactor Blackwell unit test scripts,Refactor the Blackwell unit test scripts so that they will run to completion regardless of failures and output JUnit xml for rendering passâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1667
1681,2025-09-16T00:01:07Z,perf: improve performance of cutlass fmha,fix the bug in get_unmasked_trip_count calculation in causal attention add skip correction rescale optimization in FA4.,https://github.com/flashinfer-ai/flashinfer/pull/1681
1680,2025-09-15T20:12:37Z,[TVM] Default `fixed_split_size` value in TVM binding,"This PR uses the default value -1 for fixed_split_size introduced in PR #1675, to keep the interface consistent with the TVM side.",https://github.com/flashinfer-ai/flashinfer/pull/1680
1679,2025-09-15T07:21:41Z,[misc] add a wrapper class for attention sink jit args,This PR adds a wrapper class around the JiT implementation of AttentionSink (GPT-OSS style) for FlashInfer backend BatchPrefillWithPagedKVCâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1679
1675,2025-09-15T06:40:42Z,feat: Batch-size invariant FA2 Prefill & Decode,"As mentioned in https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/, split-kv can cause non-determinism.",https://github.com/flashinfer-ai/flashinfer/pull/1675
1670,2025-09-14T03:44:49Z,feat: Add `variant.OutputTransform()` to decode kernels,I noticed that only prefill kernels use variant.OutputTransform().,https://github.com/flashinfer-ai/flashinfer/pull/1670
1662,2025-09-11T16:17:20Z,benchmark: add cupti support to benchmark,Add CUPTI support for more accurate benchmarking.,https://github.com/flashinfer-ai/flashinfer/pull/1662
1661,2025-09-11T03:21:30Z,perf&bugfix: skip kv-tile computation out of sliding window in FA2; fix __syncthreads in mergestate,"This PR skips redundant kv-tiles loading&computation in FA2's sliding window implementation, which is used by GPT-OSS inference scenarios.",https://github.com/flashinfer-ai/flashinfer/pull/1661
1664,2025-09-10T04:34:15Z,feat: Support s_qo < s_kv for prefill in flashinfer_benchmark.py and benchmark minor updates,"flashinfer_benchmark.py's attention prefill currently assumes s_qo = s_kv, which blocks measuring incremental prefill or IFB performance whâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1664
1656,2025-09-09T11:41:23Z,Add benchmark for MLARopeQuantize,Add benchmark for MLARopeQuantize,https://github.com/flashinfer-ai/flashinfer/pull/1656
1660,2025-09-09T11:38:04Z,Speedup MLARopeQuantize by 20-35%,baseline num_tokens FlashInfer 0 1.0 0.002714 1 2.0 0.003126 2 4.0 0.003328 3 8.0 0.003328,https://github.com/flashinfer-ai/flashinfer/pull/1660
1653,2025-09-09T04:45:43Z,misc: add script to analyzer code owners from git history,Help automate the process of setting up CODEOWNER,https://github.com/flashinfer-ai/flashinfer/pull/1653
1322,2025-09-08T18:25:05Z,feat: Add k_scale and v_scale to persistent attention ,For consistency with the general call in SGLang cc @yzh119,https://github.com/flashinfer-ai/flashinfer/pull/1322
1643,2025-09-06T18:01:08Z,fix: zero-init workspace buffer for trtllm-gen fmha,trtllm-gen fmha should take exclusive zero-init workspace buffer to ensure zero counter cross kernel executions.,https://github.com/flashinfer-ai/flashinfer/pull/1643
1640,2025-09-05T09:48:20Z,bugfix: Fix FLOPS calculation for bench_trtllm_gen_mla.py,Fix FLOPS calculation for bench_trtllm_gen_mla.py Details in #1638,https://github.com/flashinfer-ai/flashinfer/pull/1640
1635,2025-09-04T18:46:30Z,fix: pass workspace for trtllm-gen attention,fix: pass workspace for trtllm-gen attention,https://github.com/flashinfer-ai/flashinfer/pull/1635
1622,2025-09-04T06:03:35Z,bugfix: collect all modules to aot,Fix #1556,https://github.com/flashinfer-ai/flashinfer/pull/1622
1631,2025-09-03T22:48:21Z,bugfix: trtllm-gen fmha sm101 and sm100 compatibility,sm100 device fallback on sm101 kernels for trtllm-gen fmha.,https://github.com/flashinfer-ai/flashinfer/pull/1631
1613,2025-09-03T21:43:03Z,feat: update flashinfer-cli,"add functionality of clear cache/cubin, and show config using click library show progress bar when downloading the cubins Usage: python3 -mâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1613
1625,2025-09-03T16:37:19Z,bugfix: fix flashinfer_benchmark.py IMA when running a test list,The current flashinfer_benchmark.py script can trigger an IMA when a testlist is provided to batch-benchmark multiple test cases.,https://github.com/flashinfer-ai/flashinfer/pull/1625
1628,2025-09-03T05:39:07Z,patch mm segfault & patch cubin avail.,patch mm segfault & patch cubin avail.,https://github.com/flashinfer-ai/flashinfer/pull/1628
1614,2025-09-02T23:42:33Z,bugfix: fix merge_attention_state in BatchAttention w/ gqa-group-size in Qwen family,"This PR fixes precision issues of BatchAttention (Persistent FA2 of #1137), when CTA_TILE_Q is not a multiple of gqa_group_size (e.g., Qwenâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1614
1608,2025-09-02T20:12:01Z,"feat: initial support for SM103, SM110, SM120, SM121","feat: initial support for SM103, SM110, SM120, SM121",https://github.com/flashinfer-ai/flashinfer/pull/1608
1453,2025-08-31T07:53:06Z,feat: enable trtllm-gen attn speculative decoding verify by decode,decode with q_len > 1,https://github.com/flashinfer-ai/flashinfer/pull/1453
1503,2025-08-29T05:42:14Z,feat: integrate xqa attention backend,feat: integrate xqa attention backend,https://github.com/flashinfer-ai/flashinfer/pull/1503
1588,2025-08-27T10:52:17Z,refactor: use allocator class for workspace buffer allocation,refactor: use allocator class for workspace buffer allocation,https://github.com/flashinfer-ai/flashinfer/pull/1588
1590,2025-08-27T09:10:51Z,fix: Improve TRTLLM attention kernel out_dtype unit test,Added follow-up unit tests for #1578: passed or not passed out_dtype to the attn api.,https://github.com/flashinfer-ai/flashinfer/pull/1590
1584,2025-08-26T21:36:41Z,fix: semaphoress must be at the fixed range in workspace buffer on trtllm_gen attention,workspace_buffer arrangement on main branch paged_attention: counter (fixed 8MB at head) | scratch ragged_attention: softmax | counter (8MBâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1584
1578,2025-08-26T16:09:02Z,feat: Support for inferring out_dtype from out.dtype for TRTLLM attention kernel,"Instead of directly use query.dtype as out_dtype, we could infer the out_dtype from out.dtype if out is passed into the API.",https://github.com/flashinfer-ai/flashinfer/pull/1578
1574,2025-08-26T15:36:30Z,misc: remove some unused files,csrc/flashinfer_ops.cu and csrc/flashinfer_ops_sm90.cu are no longer used after aot refactoring.,https://github.com/flashinfer-ai/flashinfer/pull/1574
1540,2025-08-25T05:37:37Z,"feat: Add fp8-qkv, fp16/bf16 output MHA","feat: Add fp8-qkv, fp16/bf16 output MHA",https://github.com/flashinfer-ai/flashinfer/pull/1540
1567,2025-08-25T04:42:25Z,Backend: downgrade trtllm-gen kernel to cuda-12,"Downgrade trtllm-gen kernel to cuda-12, and revert #1543 (reapply #1518).",https://github.com/flashinfer-ai/flashinfer/pull/1567
1559,2025-08-24T20:22:32Z,bugfix: fix persistent attention kernel correctness on blackwell,"An explicit __syncthreads is required for guarantee correctness on blackwell, otherwise unittests will fail.",https://github.com/flashinfer-ai/flashinfer/pull/1559
1562,2025-08-24T09:51:35Z,Bugfix: some typos in Persistent kernel ,tests pass,https://github.com/flashinfer-ai/flashinfer/pull/1562
1533,2025-08-24T05:14:15Z,bugfix: Fix Persistent kernel precision for masked output ,"Previously, the packed_causal_kv_end here returns negative remaining_len, so zero_kv_len is false and these output locations are completelyâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1533
1557,2025-08-23T10:51:40Z,fix: add packaging dependency to resolve pypi workflow,"A new package packaging was introduced by #1526, which broke the pypi wrokflow.",https://github.com/flashinfer-ai/flashinfer/pull/1557
1537,2025-08-22T23:58:26Z,feat: Integrate TRTLLM varlen kernel for deepseek R1 prefill ,Integrate TRTLLM prefill kernel for deepseek R1.,https://github.com/flashinfer-ai/flashinfer/pull/1537
1543,2025-08-22T07:56:54Z,"Revert ""backend: Refactor trtllm-gen fmha metainfo loading (#1518)""",This reverts commit 7812a77.,https://github.com/flashinfer-ai/flashinfer/pull/1543
1518,2025-08-22T01:05:38Z,backend: Refactor trtllm-gen fmha metainfo loading,Update trtlln-gen fmha metainfo loading.,https://github.com/flashinfer-ai/flashinfer/pull/1518
1427,2025-08-21T16:04:53Z,refactor: Sink attention AoT,refactor: Sink attention AoT,https://github.com/flashinfer-ai/flashinfer/pull/1427
1512,2025-08-21T00:13:54Z,flashinfer_benchmark QoL Improvements and Attention FP8 Support,Current PR: Adds case_tag and generate_repro_command flags to flashinfer_benchmark.py for annotating outputs and generating reproducer commâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1512
1497,2025-08-18T20:46:08Z,benchmark: add moe to benchmark,benchmark: add moe to benchmark,https://github.com/flashinfer-ai/flashinfer/pull/1497
1496,2025-08-15T07:45:32Z,"Revert ""feat: Support fp8 qkv, fp16/bf16 out MHA for trtllm-gen. (#1490)",This reverts commit 96d142d.,https://github.com/flashinfer-ai/flashinfer/pull/1496
1492,2025-08-15T06:43:04Z,Add errors when dtype is anything other than int32 for ptr metatdata,"This PR adds a very simple error check for the metadata arguments qo_indptr, kv_indptr and kv_lens array.",https://github.com/flashinfer-ai/flashinfer/pull/1492
1490,2025-08-15T06:31:54Z,"feat: Support fp8 qkv, fp16/bf16 out MHA for trtllm-gen.","feat: Support fp8 qkv, fp16/bf16 out MHA for trtllm-gen.",https://github.com/flashinfer-ai/flashinfer/pull/1490
1460,2025-08-14T06:34:12Z,Fix TRTLLM NVFP4-out attention kernel scale factor dim issue,Fixed the shape checking for FP4 scale factor tensor.,https://github.com/flashinfer-ai/flashinfer/pull/1460
1484,2025-08-14T03:52:29Z,feat: add pdl for trtllm-gen attn,Following #1446.,https://github.com/flashinfer-ai/flashinfer/pull/1484
1328,2025-08-13T08:26:19Z,refactor: Improved metainfo for trtllm-gen kernels,Move trtllm-gen batched-gemm and gemm metainfo headers into cache directory and link when jit.,https://github.com/flashinfer-ai/flashinfer/pull/1328
1375,2025-08-13T07:26:52Z,Unify and modularize decode and prefill test.,Unify decode and prefill attention test for trtllm-gen by extract common function.,https://github.com/flashinfer-ai/flashinfer/pull/1375
1446,2025-08-13T07:24:51Z,Remove getEnvEnablePDL in favor of enable_pdl parameter,Remove getEnvEnablePDL in favor of enable_pdl parameter,https://github.com/flashinfer-ai/flashinfer/pull/1446
1463,2025-08-13T07:10:37Z,fix: remove redundant zero_init reverted by #1459,The duplicate zero_init should be fixed.,https://github.com/flashinfer-ai/flashinfer/pull/1463
1476,2025-08-13T06:38:12Z,fix: minor fix after #1384,fix: minor fix after #1384,https://github.com/flashinfer-ai/flashinfer/pull/1476
1471,2025-08-12T08:38:07Z,"Fix ""more than one operator ""/"" matches these operands""",Seeing the following error on GB200 on flashinfer/include/flashinfer/attention/mla_hopper.cuh Line 829 in fe442a2 (CAUSAL ?,https://github.com/flashinfer-ai/flashinfer/pull/1471
1384,2025-08-11T23:46:20Z,Allow BatchPrefillPagedWrapper to call cudnn API,Allow BatchPrefillPagedWrapper to call cudnn API,https://github.com/flashinfer-ai/flashinfer/pull/1384
1459,2025-08-11T09:28:25Z,"Revert ""fix: remote redundant zero_init from trtllm-gen attn (#1444)""",This reverts commit 5451029.,https://github.com/flashinfer-ai/flashinfer/pull/1459
1416,2025-08-10T10:59:43Z,Fix missing v_scale for prefill wrapper.,"We do need k,v scale !=1 for llama3 fp4 model.",https://github.com/flashinfer-ai/flashinfer/pull/1416
1434,2025-08-10T10:28:38Z,Fixes for Blackwell Tests,Fixes for Blackwell Tests,https://github.com/flashinfer-ai/flashinfer/pull/1434
1415,2025-08-10T08:30:44Z,"benchmark: trtllm-gen mha with sink, add benchmark args","[b200] Benchmark results python3 benchmarks/bench_trtllm_fmha.py batch_size=4, seq_len=1024, num_qo_heads=64, num_kv_heads=8, head_dim=64,â€¦",https://github.com/flashinfer-ai/flashinfer/pull/1415
1444,2025-08-10T08:30:17Z,fix: remote redundant zero_init from trtllm-gen attn,fix: remote redundant zero_init from trtllm-gen attn,https://github.com/flashinfer-ai/flashinfer/pull/1444
1339,2025-08-09T03:15:18Z,feat: Fused rope fp8 quantize kernel for MLA,Fusing RoPE + fp8 quantization kernel to prepare input for fp8 mla kernel.,https://github.com/flashinfer-ai/flashinfer/pull/1339
1438,2025-08-09T00:41:52Z,Putting back cudnn_batch_prefill_with_kv_cache that was deleted by ruff,Putting back cudnn_batch_prefill_with_kv_cache that was deleted by ruff,https://github.com/flashinfer-ai/flashinfer/pull/1438
1432,2025-08-08T21:57:55Z,Add NOTICE with copyrights,Adds NOTICE file to specify copyrights for both NVIDIA and Flashinfer community.,https://github.com/flashinfer-ai/flashinfer/pull/1432
1413,2025-08-08T18:23:29Z,Fix crash when pos_encoding_mode is passed as int,"This PR is to fix this issue caused by 85d75ca File ""/scratch/repo/flashinfer/flashinfer/prefill.py"", line 83, in get_fmha_module pos_encodâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1413
1390,2025-08-07T00:29:19Z,Adding FP8 benchmark on attention and matmul testing,Current PR extends benchmarking script in flashinfer_benchmark.py by adding: MLA backend testing Attention FP8 Benchmarking Introduction foâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1390
1393,2025-08-06T02:06:25Z,Add flags to trim down AoT builds,This PR trims down AoT builds to allow wheels to be generated including only the attention kernels.,https://github.com/flashinfer-ai/flashinfer/pull/1393
1389,2025-08-05T17:32:33Z,GPT-OSS Support: Add Blackwell MoE mxfp4 implementation from TRTLLM and Attention Sink,These kernels support OpenAI GPT-OSS Co-authored-by: siyuanf siyuanf@nvidia.com Co-authored-by: Zihao Ye zihaoy@nvidia.com Co-authored-by:â€¦,https://github.com/flashinfer-ai/flashinfer/pull/1389
1374,2025-08-04T09:30:38Z,Update documentation index,Update doc index,https://github.com/flashinfer-ai/flashinfer/pull/1374
1372,2025-08-04T02:42:38Z,ci: add blackwell unittest scripts,Initial setup of blackwell unittest scripts scripts/run_test_blackwell_attention_kernels.sh for moe/gemm/attention/utils/etc scripts/task_tâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1372
1369,2025-08-03T13:34:49Z,Artifact downloading and single sourced artifact path,This PR adds the features of downloading complete artifacts and makes artifacts path single sourced in python.,https://github.com/flashinfer-ai/flashinfer/pull/1369
1363,2025-08-03T00:59:49Z,Support scale factor start index for fp4 mha prefill/decode,the start index of fp4 output scale factor o_sf_start_index is useful when the decode kernels are reusing the scale factor tensor of prefilâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1363
1317,2025-08-01T21:28:46Z,Allow cudnn prefill kernels to be called natively,Allow cudnn prefill kernels to be called natively,https://github.com/flashinfer-ai/flashinfer/pull/1317
1360,2025-08-01T17:41:00Z,support trtllm-gen prefill fp4 output,Support nvfp4 for prefill function call.,https://github.com/flashinfer-ai/flashinfer/pull/1360
1283,2025-07-31T23:23:44Z,Add native cudnn_decode for improved cudnn decode performance,This PR tries to integrate cudnn decode by calling the cudnn kernels directly instead of through the cubin path.,https://github.com/flashinfer-ai/flashinfer/pull/1283
1316,2025-07-30T10:49:45Z,minor: add trtllm_gen_mla benchmark,Add a missing benchmark.,https://github.com/flashinfer-ai/flashinfer/pull/1316
1324,2025-07-30T09:44:22Z,feat: Support logits_soft_cap for Persistent attn; fix kv split limit,"When integrating this kernel into SGLang, I quickly hit an assertion error with input len 4000, output len 200 and 8 request/s due to the hâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1324
1346,2025-07-30T09:33:33Z,Add trtllm-gen prefill test. Fix related wrapper issue.,Make the old prefill test cover the direct function call and fp8.,https://github.com/flashinfer-ai/flashinfer/pull/1346
1350,2025-07-30T06:36:31Z,Support passing kv_data_type to MultiLevelCascadeAttentionWrapper.plan(),MultiLevelCascadeAttentionWrapper.plan() ends up calling plan() on BatchPrefillWithPagedKVCacheWrapper.,https://github.com/flashinfer-ai/flashinfer/pull/1350
1348,2025-07-29T18:27:20Z,fix: fix trtllm-gen mla error on new interface,Fix error introduced by #1318,https://github.com/flashinfer-ai/flashinfer/pull/1348
1318,2025-07-29T00:51:14Z,feat: support output nvfp4 in trtllm-gen function call.,"Only added fp4 support to trtllm_batch_decode_with_kv_cache, not added to wrapper yet.",https://github.com/flashinfer-ai/flashinfer/pull/1318
1267,2025-07-27T04:40:04Z,Bug fix: fix duplicate launch in POD,"Mistakenly added a duplicate kernel launch last time (actually by cursor, but should've checked more closelyðŸ˜‚) cc @yzh119",https://github.com/flashinfer-ai/flashinfer/pull/1267
1326,2025-07-25T11:18:25Z,Fix redundant argument in TrtllmGenDecodeModule,Remove the redundant sm_count argument in the TrtllmGenDecodeModule which takes window_left position.,https://github.com/flashinfer-ai/flashinfer/pull/1326
1323,2025-07-25T00:02:03Z,Addition of flashinfer_benchmark.py for benchmarking routines,Adds benchmarks/flashinfer_benchmark.py and utility functions for benchmarking performance of various FI APIs.,https://github.com/flashinfer-ai/flashinfer/pull/1323
1290,2025-07-24T18:03:08Z,[fix] fix integer overflow in FA2 customized_mask & add buffer overflow warning.,"Per discussion with @haochengxi and @Radioheading, this PR moves the plan function in VariableBlockSparseAttentionWrapper to the GPU side,â€¦",https://github.com/flashinfer-ai/flashinfer/pull/1290
1314,2025-07-24T10:55:35Z,test qkvo quantization not equal to 1.,test qkvo quantization not equal to 1.,https://github.com/flashinfer-ai/flashinfer/pull/1314
1307,2025-07-23T08:33:23Z,Fix the bug of the kernel-selection heuristic in trtllm-gen,this fixes the bug of still selecting low-latency (swapsMmaAb) MLA kernels when batch size is quite large under the high-throughput case (aâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1307
1305,2025-07-23T02:39:14Z,[Feature] SM level profiler ,Simply add smid into the profiler tag.,https://github.com/flashinfer-ai/flashinfer/pull/1305
1302,2025-07-23T00:43:55Z,minor: some fix and cleanup for trtllm-gen mha,minor: some fix and cleanup for trtllm-gen mha,https://github.com/flashinfer-ai/flashinfer/pull/1302
1289,2025-07-22T17:25:24Z,refactor: refactor trtllm-gen attention kernel integration code,"Simplify and unify the interface for trtllm-gen decode/prefill/mla kernels, and add support for shared-kv (in MLA, #1273).",https://github.com/flashinfer-ai/flashinfer/pull/1289
1295,2025-07-21T23:11:31Z,fix: minor errors in cubin loader,fix: minor errors in cubin loader,https://github.com/flashinfer-ai/flashinfer/pull/1295
1292,2025-07-21T19:19:38Z,refactor: Improved metainfo for trtllm-gen fmha,Refactor the metainfo for trtllm gen fmha.,https://github.com/flashinfer-ai/flashinfer/pull/1292
1286,2025-07-18T17:01:14Z,fix multiCtasKvScratchPtr misalignment issue (new one),fix multiCtasKvScratchPtr misalignment issue num_semaphores is rounded up to multiple of 8 to since the kernels require 16B alignment for mâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1286
1280,2025-07-18T09:00:13Z,fix: update trtllm-gen fmha benchmark,fix: update trtllm-gen fmha benchmark,https://github.com/flashinfer-ai/flashinfer/pull/1280
1239,2025-07-17T10:31:55Z,add trtllm-gen context attention,Integrate trtllm-gen context attention.,https://github.com/flashinfer-ai/flashinfer/pull/1239
1242,2025-07-16T17:50:19Z,Add trtllm-gen attention mha kernel with FP8 Q/K/V and FP8 output,Support TRT-LLM gen FP8 Q kernel with FP4/FP8 output,https://github.com/flashinfer-ai/flashinfer/pull/1242
1258,2025-07-16T10:58:21Z,feat: enable trtllm-gen mla MTP,Enable deepseek MTP with q>1.,https://github.com/flashinfer-ai/flashinfer/pull/1258
1214,2025-07-16T10:11:07Z,Feature/sm100 low latency nvfp4 kernels,Enable Blackwell with speed of light low latency kernels.,https://github.com/flashinfer-ai/flashinfer/pull/1214
1254,2025-07-15T13:24:42Z,fix: correctly pass k_scale and v_scale to run() in forward_return_lse (#1023),Summary This PR fixes a bug in BatchPrefillWithPagedKVCacheWrapper.forward_return_lse() where k_scale and v_scale were incorrectly passed aâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1254
1251,2025-07-15T07:18:56Z,Reduce the JIT compilation time of gen_gemm_sm100_module,Reduce the JIT compilation time of gen_gemm_sm100_module,https://github.com/flashinfer-ai/flashinfer/pull/1251
1222,2025-07-14T06:54:41Z,feat: add trtllm-gen mla cubin,Add trtllm-gen mla cubin.,https://github.com/flashinfer-ai/flashinfer/pull/1222
1234,2025-07-08T17:13:24Z,bugfix: support uint8_t for vec_t class template,This PR tries to fix an issue that occured while enabling fp8 kv-cache support for vllm (vllm-project/vllm#17005).,https://github.com/flashinfer-ai/flashinfer/pull/1234
1221,2025-07-08T07:27:42Z,Enable cudnn decode and add tests for the cudnn decode kernel,Enable cudnn decode kernel API,https://github.com/flashinfer-ai/flashinfer/pull/1221
1230,2025-07-08T06:53:45Z,feat: Add non-causal cudnn prefill kernels,Allow non-causal kernels of cudnn in cubin.,https://github.com/flashinfer-ai/flashinfer/pull/1230
1227,2025-07-07T23:56:35Z,Fix missing hash in the cudnn cubin path,Update the missing hash that is required,https://github.com/flashinfer-ai/flashinfer/pull/1227
1189,2025-07-07T16:54:24Z,update trtllm-gen decode attention kernel launcher,"This PR updates the trtllm gen kernels for most decoding cases, like head group size from 1 to 8.",https://github.com/flashinfer-ai/flashinfer/pull/1189
1198,2025-07-06T22:45:24Z,bugfix: fix blackwell fmha hanging issue for empty kv_len,Cherry picked from cutlass v4.0 changes.,https://github.com/flashinfer-ai/flashinfer/pull/1198
1217,2025-07-05T19:59:03Z,[TVM] Remove `enable_pdl` from TVM binding interface,"As of now PDL is not introduced in the TVM attention kernel interface, so we remove it from here and use false by default.",https://github.com/flashinfer-ai/flashinfer/pull/1217
1208,2025-07-03T05:59:56Z,Fix the issue with auxillary kernel launch and grid dim calculation,Fixes an issue where the auxillary kernel launch was incorrect causing kernel hangs in some cases.,https://github.com/flashinfer-ai/flashinfer/pull/1208
1206,2025-07-01T23:38:46Z,[fix] fix BatchAttention CTA_TILE_KV mask issue,The mismatch between kv_chunk_size and CTA_TILE_KV could lead to unmask nan value during small batch inference.,https://github.com/flashinfer-ai/flashinfer/pull/1206
1187,2025-07-01T02:38:52Z,Feature/cudnn dynamic cubin,Enable cudnn cubin for deepseek prefill models.,https://github.com/flashinfer-ai/flashinfer/pull/1187
1200,2025-06-30T22:10:26Z,[feat] optimize persistent batch attention perf.,"This is a follow-up PR on #1137, optimizing scheduling balance & reduction overhead, and achieving 2x speedup.",https://github.com/flashinfer-ai/flashinfer/pull/1200
1181,2025-06-27T18:18:16Z,bugfix: fix invalid blackwell fmha unittests,"Some of the unittests for fmha varlen (non-contiguous + head dim (192,128)) are not invalid.",https://github.com/flashinfer-ai/flashinfer/pull/1181
1177,2025-06-27T00:37:57Z,[feat] support block sparse attention w/ variable block sizes and head-wise sparse patterns,"This PR implements a block sparse attention wrapper that calls the underlying FA2 and FA3 kernel implementation, which supports: Variable bâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1177
1170,2025-06-25T02:15:46Z,feat: logits processor fustion rule for temperature softmax,Add fustion rule to apply recently implemented fused temperature + softmax kernel in #1153.,https://github.com/flashinfer-ai/flashinfer/pull/1170
1153,2025-06-24T04:43:27Z,feat: Fused temperature online softmax kernel,This adds the fused temperature online softmax kernel to the sampling and logits processor API.,https://github.com/flashinfer-ai/flashinfer/pull/1153
1159,2025-06-22T03:23:50Z,feat: add finalize_moe_allreduce from trtllm,add finalize_moe_allreduce from trtllm,https://github.com/flashinfer-ai/flashinfer/pull/1159
1158,2025-06-19T20:20:49Z,Add more logging to TRTLLM-GEN debug trace (NFC),This is meant to help debugging the lookup/launch sequence for these kernels.,https://github.com/flashinfer-ai/flashinfer/pull/1158
1148,2025-06-16T15:29:40Z,[fix] fix precision errors when applying causal mask on Qwen-2.5 series models,This PR identifies and fixes a precision error that occurs (rarely but possibly) on models with non-power-of-two gqa_group_size when applyiâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1148
1146,2025-06-16T06:43:37Z,misc: remove sync between persistent runners and use packed_causal_kv_end for SM90Plan,misc: remove sync between persistent runners and use packed_causal_kv_end for SM90Plan,https://github.com/flashinfer-ai/flashinfer/pull/1146
1140,2025-06-13T03:51:00Z,Fix FA2 and FA3 multi-item scoring and cuda illegal memory access error,This PR fixes FA2 and FA3 multi-item scoring and resolves CUDA illegal memory access errors in the FlashInfer attention kernels.,https://github.com/flashinfer-ai/flashinfer/pull/1140
1137,2025-06-12T05:12:12Z,[feat] add unified batch attention w/ correctness tests.,"Follow up of #858, #967, and #1026, this PR aims to provide an efficient and unified API for processing prefill and decode requests withinâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1137
1136,2025-06-11T20:12:32Z,fix: negative zero by type trait --> binary value,We are going to fix the has_neg_zero.,https://github.com/flashinfer-ai/flashinfer/pull/1136
1117,2025-06-10T22:02:10Z,[Feature] Support PDL for batch Prefill and Decode,"Add PDL support for batched prefill (FA2), decode, merge states and MLA decode, overlapping local variable and state init (e.g.",https://github.com/flashinfer-ai/flashinfer/pull/1117
1129,2025-06-08T16:19:28Z,Fix pointer dtype bug in rope,According to sgl-kernel(https://github.com/sgl-project/sglang/blob/c2c4f57f6311ba143c6156ab1d1a1d9413e6e4d0/sgl-kernel/README.md?plain=1#L1â€¦,https://github.com/flashinfer-ai/flashinfer/pull/1129
1116,2025-06-06T02:58:03Z,hotfix: fix the blackwell fmha stream,"This PR is a followup of #1106 , however, sglang integration of blackwell blackwell fmha still do not reach desired performance (sgl-projecâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1116
1109,2025-06-03T20:26:05Z,bugfix: bugfix for blackwell mla split-k,Cherry-picked the patch (authored by @v0i0) to fix #1055,https://github.com/flashinfer-ai/flashinfer/pull/1109
1106,2025-06-03T00:40:17Z,bugfix: host-precomuted plan function for blackwell fmha,"Accelerate blackwell causal fmha, and fix issue #1103",https://github.com/flashinfer-ai/flashinfer/pull/1106
1051,2025-05-27T01:22:00Z,[nvidia] Add Blackwell FMHA decode kernel from TRT-LLM,This patch adds support for downloading cubins on the fly and caching them on disk.,https://github.com/flashinfer-ai/flashinfer/pull/1051
1093,2025-05-26T03:21:46Z,misc: followup,Followup of #1092 The last one.,https://github.com/flashinfer-ai/flashinfer/pull/1093
1087,2025-05-23T22:18:37Z,bugfix: fix fp8 attention kernels aot compilation issue,"batch_indices_offset (introduced in #1015 ) are not passed to fp8 attention kernels, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/1087
1067,2025-05-22T01:23:03Z,misc: aot: Add script to build all AOT ops,Part of AOT Refactor (#1064).,https://github.com/flashinfer-ai/flashinfer/pull/1067
1073,2025-05-22T00:48:40Z,misc: jit: Import jit_env as a module,Part of AOT Refactor (#1064).,https://github.com/flashinfer-ai/flashinfer/pull/1073
1072,2025-05-20T15:57:14Z,bugfix: follow user-specified sm_scale for blackwell cutlass fmha,Use user-specified instead of hardcoded sm_scale for blackwell cutlass fmha kernel.,https://github.com/flashinfer-ai/flashinfer/pull/1072
1071,2025-05-20T06:12:09Z,bugfix: adding lse output to blackwell fmha kernels,bugfix: adding lse output to blackwell fmha kernels,https://github.com/flashinfer-ai/flashinfer/pull/1071
1059,2025-05-14T22:34:15Z,Parameterize prefix mask call (needed by POD-Attention),Passes the threadId to the prefix mask call manually.,https://github.com/flashinfer-ai/flashinfer/pull/1059
1055,2025-05-13T07:05:41Z,bugfix: temporally disable split-kv in blackwell mla,"There are some bugs with blackwell mla when split-kv is enabled, temporally disable it to guarantee the correctness of results.",https://github.com/flashinfer-ai/flashinfer/pull/1055
1054,2025-05-13T05:18:45Z,Fix KV chunking for POD. ,"For some reason cudaOccupancyMaxActiveBlocksPerMultiprocessor returns 0, so manually calculate the value instead.",https://github.com/flashinfer-ai/flashinfer/pull/1054
1039,2025-05-13T02:06:29Z,[nvidia] initial support for blackwell kernels,Mainly adapted from cutlass examples.,https://github.com/flashinfer-ai/flashinfer/pull/1039
1052,2025-05-11T20:27:19Z,Benchmark: POD vs batched prefill,Supersedes yzh119#5 python benchmarks/bench_mixed_attention.py,https://github.com/flashinfer-ai/flashinfer/pull/1052
1015,2025-04-30T18:49:55Z,add multi-item scoring,Co-authored with Qingquan Song (@qingquansong) and Ziang Li (@zianglih ) Multi-item scoring concatenate multiple candidates of a same membeâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1015
1033,2025-04-29T21:25:18Z,feat: add functional per-head FP8 quantization for FA3,This PR adds FP8 support in FA3 to speed up compute-bound prefill kernels.,https://github.com/flashinfer-ai/flashinfer/pull/1033
1040,2025-04-25T15:58:07Z,Non-blocking host-to-device copy in the ragged prefill wrapper,Non-blocking host-to-device copy in the ragged prefill wrapper,https://github.com/flashinfer-ai/flashinfer/pull/1040
1031,2025-04-23T16:54:31Z,[NVIDIA] Add Cutlass MLA backend,This PR add a cutlass backend to the flashinfer BatchMLAPagedAttentionWrapper.,https://github.com/flashinfer-ai/flashinfer/pull/1031
1029,2025-04-22T04:36:15Z,fix: add zero init for KV tiled copy,"This PR fixes the out-of-bound K/V value loading in the FA3 sparse_mainloop.cuh, which may cause a nan value for BatchPrefillWithPagedKVCacâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1029
1028,2025-04-21T16:12:38Z,bugfix: fix custom mask not be reseted after convert custom mask into causal or non-causal,"When using custom mask for Medusa or other method, flashinfer api will inference with custom mask buf, but when there has no draft token, tâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1028
1007,2025-04-15T16:30:43Z,feat: update decode attention APIs,This PR updates the decode attention APIs by: Adding lse return value for single_decode_with_kv_cache API per requested in #1006 (note thatâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/1007
1013,2025-04-15T14:34:33Z,bugfix: import wrapper of mla decode,"In tests/test_mla_decode_kernel.py, the code below uses BatchDecodeMlaWithPagedKVCacheWrapper: flashinfer/tests/test_mla_decode_kernel.py Lâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/1013
1014,2025-04-10T23:16:02Z,misc: fix instrument code for mla profiler,"Follow up of #952 , this PR adds the instrument code base to profile mla hopper implementation (fix #995 ) cc @ziyuhuang123",https://github.com/flashinfer-ai/flashinfer/pull/1014
982,2025-04-01T19:36:00Z,SM-constraint-GEMM by triton persistent kernel,Add SM-constraint GEMM operation by triton persistent kernel to support Nanoflow infra-device parallelism.,https://github.com/flashinfer-ai/flashinfer/pull/982
991,2025-03-31T21:27:39Z,perf: prefetch page indices for mla kernel,"Followup of #952 cc @abcdabcd987 Before this PR Config: batch_size=64, seq_len=1024, num_heads=64 Memory bandwidth: 1509.87 GB/s FLOPs: 163â€¦",https://github.com/flashinfer-ai/flashinfer/pull/991
952,2025-03-29T04:34:14Z,perf: Use 2WG pipeline design for MLA implementation on Hopper,This PR implements #892 .,https://github.com/flashinfer-ai/flashinfer/pull/952
966,2025-03-22T08:24:44Z,doc: remove misleading docstring about `non_blocking`,"As noted in #965 , we have some misleading docstring about the use of non_blocking option in plan functions of attention wrappers (they areâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/966
961,2025-03-19T16:57:53Z,Fix compilation on cuda 12.2,Compiling FlashInfer on CUDA 12.2 triggers errors such as those shown below.,https://github.com/flashinfer-ai/flashinfer/pull/961
943,2025-03-18T16:33:09Z,ci: improve jenkins,cancel previous build if new commit comes add task 3 accelerate rope test Co-authored-by: Yong Wu yowu@nvidia.com,https://github.com/flashinfer-ai/flashinfer/pull/943
956,2025-03-17T21:51:51Z,misc: Temporarily disable POD from AOT wheels,We currently don't generate the AOT implementations for POD Attention.,https://github.com/flashinfer-ai/flashinfer/pull/956
951,2025-03-17T16:28:40Z,bugfix: bugfix to #949,"The sm86/sm89 version of mla kernel was not tests after change #942, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/951
954,2025-03-17T15:00:09Z,fix: fix pod-attention compilation time,"As mentioned in #953 , we shouldn't include <flashinfer/attention/pod.cuh> in pod_tensor.cu (now pod.cu).",https://github.com/flashinfer-ai/flashinfer/pull/954
942,2025-03-15T02:35:14Z,fix - fix bug when not relevant seq has nan data,"We found that in the batch MLA FA2 implementation, if there exists NaN data within the computed CKV cache page, the kernel implementation wâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/942
945,2025-03-14T18:53:36Z,bugfix: fix potential issues of FA3 template loading nans for PageAttention,Ref #941,https://github.com/flashinfer-ai/flashinfer/pull/945
858,2025-03-13T20:54:28Z,Add POD-Attention to FlashInfer,Adds POD-Attention kernel (https://arxiv.org/abs/2410.18038) with all necessary files.,https://github.com/flashinfer-ai/flashinfer/pull/858
888,2025-03-13T18:23:08Z,feat - support mla kvcache store,Summary related to #877 This PR implement MLA cache storeï¼Œand passed correctness test in the case of ckv_dim=512 and kpe_dim=64ï¼Œbut no furtâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/888
940,2025-03-13T16:19:23Z,fix: Fix MLA TVM binding for the latest changes,This PR applies changes in #898 and #900 to the MLA TVM binding.,https://github.com/flashinfer-ai/flashinfer/pull/940
913,2025-03-06T00:46:43Z,feat: flashinfer intra-kernel profiler,"FlashInfer Profiler is a tool for intra-kernel profiling for diagnosing kernel performance, by visualizing the work of each CTA/warpgroup.",https://github.com/flashinfer-ai/flashinfer/pull/913
904,2025-02-28T16:14:00Z,bugfix: Fix no return type error,This pull request includes a change to the convert_s_to_p function in the mla_hopper.cuh file to improve type safety and clarity.,https://github.com/flashinfer-ai/flashinfer/pull/904
869,2025-02-27T21:29:57Z,Naive Support for Hopper FP8 Prefill Kernel with Per-Head Quantization,Summary This PR introduces naive FP8 tensor core computation following FA3's implementations.,https://github.com/flashinfer-ai/flashinfer/pull/869
902,2025-02-27T05:59:18Z,release: bump version v0.2.2.post1,Fix several performance bugs in MLA kernel.,https://github.com/flashinfer-ai/flashinfer/pull/902
901,2025-02-27T05:55:29Z,perf: tweak the pipeline design of mla kernel,defer barrier sync for p_smem change unroll number from 1 to 2 We found there are still significant overhead for synchronizing two consumerâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/901
900,2025-02-27T04:00:22Z,perf: use f16 as split-k partial output data type,"We use f32 as split-k partial output data type, mainly for accuracy concern.",https://github.com/flashinfer-ai/flashinfer/pull/900
898,2025-02-26T01:32:04Z,perf: fix MLA split-k performance bug,"As observed in #892 , we found flashinfer mla's second stage of split-k is very slow (when batch size is small), this is because our scheduâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/898
896,2025-02-24T17:41:10Z,perf: tweak register amount for producer/consumer in MLA template,"56, 224 is faster.",https://github.com/flashinfer-ai/flashinfer/pull/896
895,2025-02-24T15:57:36Z,fix: pin_memory use cpu as default device,Use cpu as default device for pin_memory in mla.py and sparse.py,https://github.com/flashinfer-ai/flashinfer/pull/895
891,2025-02-23T22:28:08Z,bump version to v0.2.2,flashinfer now supports fa3 version of MLA kernel.,https://github.com/flashinfer-ai/flashinfer/pull/891
890,2025-02-23T20:35:21Z,unittest: add unittests for MLA + cudagraph,This update also addresses an issue in the scheduler that could cause the program to hang under certain conditions.,https://github.com/flashinfer-ai/flashinfer/pull/890
889,2025-02-23T19:08:49Z,[JIT] Fix MLA header in TVM binding,"This PR fixes the header include, following changes in #887.",https://github.com/flashinfer-ai/flashinfer/pull/889
887,2025-02-23T11:37:57Z,perf: FlashAttention-3 style MLA PageAttention,"This PR is the followup of #804 , we implemented a FlashAttention-3 version of warp specialization pattern (splitting on head-dimension) inâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/887
881,2025-02-20T05:51:30Z,[Hotfix] Add flashinfer.jit.attention into packages,"flashinfer.jit.attention package is introduced in #880, but it's not added into the packages list when building wheel.",https://github.com/flashinfer-ai/flashinfer/pull/881
880,2025-02-19T20:32:44Z,JIT compilation support for TVM,"This PR introduces the FlashInfer JIT compilation for TVM, with corresponding TVM bindings.",https://github.com/flashinfer-ai/flashinfer/pull/880
878,2025-02-19T18:13:50Z,misc:Remove unused k_smem_offset_w update in MLA kernel,"The variable ckv_smem_offset_w and kpe_smem_offset_w are never used after update in current loop, and they will be redefined and recomputedâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/878
868,2025-02-17T21:56:32Z,bugfix: fix the behavior of MLA kernel when kv-length is 0,"The scheduling algorithm in #863 do not consider some requests have kv-cache length 0, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/868
861,2025-02-17T19:39:48Z,unittest: add MLA test cases where kv_len is evenly divided by page_size.,@yzh119 PLZ take a look for this test.,https://github.com/flashinfer-ai/flashinfer/pull/861
863,2025-02-17T15:30:20Z,perf: dynamic split-k for MLA,"#804 didn't implement split-k, which might result in performance degradation if concurrency is not large enough.",https://github.com/flashinfer-ai/flashinfer/pull/863
856,2025-02-16T16:31:44Z,typo: update `decode_maybe_q_rope_offset`,"Following up on #855 and #847, this PR fixes a typo of inline RoPE in decode kernel.",https://github.com/flashinfer-ai/flashinfer/pull/856
855,2025-02-16T16:02:16Z,Unique the symbol of maybe_q_rope_offset_v.,"Having building error with multiple definitions of has_maybe_q_rope_offset_v with building command: TORCH_CUDA_ARCH_LIST=""9.0a"" FLASHINFER_â€¦",https://github.com/flashinfer-ai/flashinfer/pull/855
850,2025-02-16T07:22:46Z,misc: Remove duplicate param set in MLA kernel,This PR removes the duplicate set of params.kv_indices in the MLA kernel.,https://github.com/flashinfer-ai/flashinfer/pull/850
847,2025-02-16T02:59:47Z,bugfix: Fix inline RoPE in decode kernels,This PR fixes a bug in decode kernel that fails to process the q_rope_offset provided in Params.,https://github.com/flashinfer-ai/flashinfer/pull/847
808,2025-02-16T00:37:59Z,fix: Pass backend in BatchPrefillWith*KVCacheWrapper.plan(),"Fix #807 For the BatchPrefillWith*KVCacheWrapper that support both FA2 and FA3, user may call the ctor() once and the plan() multiple times.",https://github.com/flashinfer-ai/flashinfer/pull/808
844,2025-02-14T17:45:14Z,perf: MLA decode kernel implemented by CuTe targeted to SM80,"Hi @yzh119 , this is a follow up of #766, an interesting idea came to my mind today, can't help to change few lines to verify this idea.",https://github.com/flashinfer-ai/flashinfer/pull/844
822,2025-02-13T12:12:37Z,hotfix: bugfix on #812,"We met some very weird issues, might be a nvcc bug.",https://github.com/flashinfer-ai/flashinfer/pull/822
821,2025-02-13T10:15:04Z,bugfix: bugfix on sm89 MLA,"Follow up of #814 , we found some correctness issue of sm89 MLA kernels, this PR fixes them.",https://github.com/flashinfer-ai/flashinfer/pull/821
764,2025-02-13T08:33:11Z,refactor: change to TORCH_LIBRARY,draft pr,https://github.com/flashinfer-ai/flashinfer/pull/764
818,2025-02-13T07:59:54Z,doc: improve mla related documentation,doc: improve mla related documentation,https://github.com/flashinfer-ai/flashinfer/pull/818
816,2025-02-13T07:16:31Z,bugfix: fix the behavior of mla plan function when provided with host tensors,cc @abcdabcd987,https://github.com/flashinfer-ai/flashinfer/pull/816
814,2025-02-12T21:17:17Z,feat: unlock MLA attention for sm89 (L40/L40s/4090),"This PR changes the MLA attention template to support sm89 GPUs, which has small shared memory size (99kb per sm), so we have to further reâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/814
813,2025-02-12T18:51:36Z,feat: cudagraph-compatible MLA API,feat: cudagraph-compatible MLA API,https://github.com/flashinfer-ai/flashinfer/pull/813
812,2025-02-12T18:21:34Z,feat: unlocking MLA for A100,Generalize the MLA template to support different CTA_TILE_KV and NUM_STAGES for GPUs such as A100.,https://github.com/flashinfer-ai/flashinfer/pull/812
811,2025-02-12T07:39:49Z,doc: add documentation to new MLA interface,Add new MLA API to documentation.,https://github.com/flashinfer-ai/flashinfer/pull/811
810,2025-02-12T06:55:49Z,bugfix: mla page-attention kernel for different page sizes,"The previous PR #804 only tests page_size 1, this PR fixes the issue with other page sizes and add corresponding unittests.",https://github.com/flashinfer-ai/flashinfer/pull/810
804,2025-02-12T05:02:18Z,perf: memory efficient deepseek mla fused page-attention kernel,"This PR implements a memory efficient fused Deepseek MLA PageAttention kernel for decode, prefill, and chunked prefill operations.",https://github.com/flashinfer-ai/flashinfer/pull/804
803,2025-02-11T18:26:47Z,"bugfix: mla decode failed under cuda graph mode, and update test case","bugfix: mla decode failed under cuda graph mode, and update test case",https://github.com/flashinfer-ai/flashinfer/pull/803
799,2025-02-08T20:15:11Z,feat: support f32 attention output in FA2 template,"For bf16 kernels, we need to use f32 as the intermediate data type for split-k partial output to avoid numerical errors.",https://github.com/flashinfer-ai/flashinfer/pull/799
798,2025-02-08T08:26:49Z,Fix the type annotation of q_dtype and kv_dtype on ragged prefill,Fix the type annotation of q_dtype and kv_dtype on ragged prefill,https://github.com/flashinfer-ai/flashinfer/pull/798
797,2025-02-07T17:49:18Z,test: add unittest comparing deepseek prefill fa2 & 3 implementation,test: add unittest comparing deepseek prefill fa2 & 3 implementation,https://github.com/flashinfer-ai/flashinfer/pull/797
795,2025-02-07T16:48:13Z,Fix arguments of `plan` for split QK/VO head dims,"#765 introduced changes to the API of plan, including renaming head_dim to head_dim_qk and adding head_dim_vo.",https://github.com/flashinfer-ai/flashinfer/pull/795
793,2025-02-07T07:00:34Z,fix rope logic in mla decoding,"Co-authored-by: pankajroark pankajroark@users.noreply.github.com As titled, unblock the FlashInfer integration.",https://github.com/flashinfer-ai/flashinfer/pull/793
787,2025-02-05T16:00:21Z,bugfix: MLA decode should multiply sm_scale by math::log2e,"After this fix, the mse_use_flashinfer in test_mla_decode_kernel.py will improve significantly.",https://github.com/flashinfer-ai/flashinfer/pull/787
781,2025-02-04T19:14:15Z,bugfix: fix batch prefill attention kernel unittests,This pull request includes a small change to the tests/test_batch_prefill_kernels.py file.,https://github.com/flashinfer-ai/flashinfer/pull/781
778,2025-02-04T06:56:26Z,feat: Separate QK/VO head dim dispatch for sm90 AOT,"#765 introduces hacks to support DeepSeek head dims, but AOT is broken.",https://github.com/flashinfer-ai/flashinfer/pull/778
776,2025-02-04T05:20:55Z,perf: refactor fa2 prefill template,"This PR refactors the FA2-based prefill template, including the following changes: Using KernelTraits for all constexpr and data types.",https://github.com/flashinfer-ai/flashinfer/pull/776
772,2025-02-01T05:24:23Z,refactor: change the structure of attention updater,"Per discussion with @happierpig , user should be able to construct attention updater in their own way (and parameters).",https://github.com/flashinfer-ai/flashinfer/pull/772
765,2025-02-01T03:44:52Z,feat: support deepseek prefill attention shape,"Deepseek requires head_dim_qk of 192 and head_dim_vo of 128, this PR implements this feature for prefill attention on ragged tensors.",https://github.com/flashinfer-ai/flashinfer/pull/765
768,2025-01-31T19:48:42Z,Version bump: v0.2.0.post2,Release a version before v0.2.1 (which includes a bunch of mla updates).,https://github.com/flashinfer-ai/flashinfer/pull/768
767,2025-01-31T19:43:06Z,bugfix: Fix block-sparse attention API,This pull request includes a small change to the flashinfer/sparse.py file.,https://github.com/flashinfer-ai/flashinfer/pull/767
754,2025-01-27T06:09:37Z,Change `apply_rope_with_cos_sin_cache` to accept `cos_sin_cache`,"For sgl-project/sglang#3134 Problem: In SGLang, we pass cos_sin_cache with the shape (max_len, rot_dim) to RoPE kernel.",https://github.com/flashinfer-ai/flashinfer/pull/754
755,2025-01-27T03:18:58Z,fix pin memory device,right now flashinfer does not specify the device when creating pin-memory tensor.,https://github.com/flashinfer-ai/flashinfer/pull/755
753,2025-01-26T15:54:31Z,[bugfix] Fix cpp tests/benchmarks,Create MLA instantiations Set use_sliding_window=False in benchmarks.,https://github.com/flashinfer-ai/flashinfer/pull/753
752,2025-01-25T04:10:21Z,bugfix: various AOT issues,Add missing instantiation for batch prefill and single prefill.,https://github.com/flashinfer-ai/flashinfer/pull/752
751,2025-01-25T02:18:43Z,Filter out unsupported head dim for sm90,"Ran into compilation error when setting FLASHINFER_HEAD_DIMS=64,128,192,256.",https://github.com/flashinfer-ai/flashinfer/pull/751
748,2025-01-23T09:09:13Z,[Refactor] Unify JIT/Customization/AOT mode,This PR implements the #706 to unify the codebase for (1) JIT compilation of default attention (2) JIT compilation of customized attentionâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/748
714,2025-01-04T07:31:23Z,perf: fix the iteration bound of SWA in FA2 prefill template,"We forgot to divide the packed row index by group_size when computing the sliding window iteration bound, making it larger than its actualâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/714
704,2024-12-29T17:03:41Z,Customizable SM90 prefill kernels.,Added customizable SM90 prefill kernels.,https://github.com/flashinfer-ai/flashinfer/pull/704
697,2024-12-25T07:54:54Z,bugfix: casting int array to int32 for rope input arguments,To avoid the potential bugs when user pass LongTensor to rope APIs.,https://github.com/flashinfer-ai/flashinfer/pull/697
691,2024-12-23T00:48:52Z,release: bump version to v0.2.0.post1,0.2.0.post1 (2024-12-22) Bug Fixes bug fix on determine_attention_backend condition (#688) (bcf7a3e) accelerate plan speed of fa3 templateâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/691
690,2024-12-23T00:39:35Z,hotfix: accelerate plan speed of fa3 template,"The fa3 template's plan speed is very slow because we overestimate the workspace size that needs to be transferred from CPU to GPU, this PRâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/690
688,2024-12-20T07:29:50Z,bugfix: bug fix on `determine_attention_backend` condition,Should only enable fa3 for cuda 12.3+,https://github.com/flashinfer-ai/flashinfer/pull/688
476,2024-12-17T12:59:05Z,chore(main): release 0.2.0,ðŸ¤– I have created a release beep boop 0.2.0 (2024-12-17) Release Blog.,https://github.com/flashinfer-ai/flashinfer/pull/476
676,2024-12-17T12:15:57Z,doc: expose MLA decode API to documentation,BatchDecodeMlaWithPagedKVCacheWrapper is not indexed.,https://github.com/flashinfer-ai/flashinfer/pull/676
672,2024-12-17T07:27:40Z,feat: add JIT compilation support for FA3 templates,Follow up work of #667,https://github.com/flashinfer-ai/flashinfer/pull/672
670,2024-12-17T05:25:18Z,bugfix: fix JIT compilation of batch prefill attention kernels,The batch prefill attention JIT template was broken in #635 because we messed up some jinja syntax.,https://github.com/flashinfer-ai/flashinfer/pull/670
667,2024-12-16T13:08:53Z,perf: Dense and sparse customizable flashattention-3 template,This PR adds flashattention-3 template for improving prefill performance on hopper.,https://github.com/flashinfer-ai/flashinfer/pull/667
662,2024-12-14T09:28:39Z,ci: cross python wheel,We can build a single python wheel for all supported python versions.,https://github.com/flashinfer-ai/flashinfer/pull/662
650,2024-12-05T17:48:52Z,Fix MLA decode error having v_scale without return_lse,"[rank0]: File ""/home/simonmo/.conda/envs/vllm/lib/python3.10/site-packages/flashinfer/decode.py"", line 1408, in run [rank0]: out[0] *= v_scâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/650
644,2024-12-04T10:27:50Z,Reduce total_num_tiles_q by one,The bound can be reduced by one to slightly decrease workspace memory usage.,https://github.com/flashinfer-ai/flashinfer/pull/644
640,2024-11-26T08:38:13Z,Fix the batch size/seq len args for the decode kernel with tensor cores,"A prior PR switched up the two arguments via a typo, without tests catching the problem.",https://github.com/flashinfer-ai/flashinfer/pull/640
639,2024-11-25T19:10:52Z,Fix the maximal grid dimension in prefill planning with CUDA graphs,"Previously, differences in the contents of qo_indptr could lead to block sizes varying across CUDA graph invocations, leading to illegal meâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/639
633,2024-11-24T10:56:31Z,bugfix: fix sliding window attention tests of ragged attention api,The unittests of sliding window atteniton didn't pass because we forget to set causal in plan function.,https://github.com/flashinfer-ai/flashinfer/pull/633
632,2024-11-24T10:38:28Z,perf: speedup jit compilation of prefill attention kernels,"Followup of #628, this PR splits prefill attention jit templates so that we compile different mask modes in different files.",https://github.com/flashinfer-ai/flashinfer/pull/632
628,2024-11-23T22:39:46Z,jit: further accelerate compilation by spliting files and multi-threading,This PR accelerates JIT compilation by: Add a parallel_load_modules function to load necessary modules for a model in parallel using pythonâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/628
625,2024-11-21T10:42:07Z,bugfix: fix append_paged_kv_cache test,bugfix: fix append_paged_kv_cache test,https://github.com/flashinfer-ai/flashinfer/pull/625
624,2024-11-21T01:09:28Z,bugfix: fix prefill kernel uris for aot compilation,"mask is no longer part of uris, this PR fixes the issue, otherwise our aot wheels will still trigger JIT compilation for prefill kernels.",https://github.com/flashinfer-ai/flashinfer/pull/624
620,2024-11-20T09:58:17Z,bugfix: fix MLA with new JIT pipeline,"Some of the commits for fixing MLA are missing in #618, this PR add them back.",https://github.com/flashinfer-ai/flashinfer/pull/620
619,2024-11-20T09:43:46Z,bugfix: fix the rope correctness issue introduced in #609,"As observed by @james-p-xu, #609 produce wrong results for some input shapes, this PR fixes the correctness issue, and add optimizations ofâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/619
613,2024-11-18T19:43:08Z,Fix compile error of OptionalCUDAGuard and device_of,"There are some compile errors in the main branch, like /app/python/csrc_aot/single_prefill.cu(59): error: namespace ""at::cuda"" has no membeâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/613
611,2024-11-15T19:23:57Z,misc: add device guard for kernels,plan Check all kernels and add device guard Complete the tests FIX: #452,https://github.com/flashinfer-ai/flashinfer/pull/611
609,2024-11-14T07:46:16Z,Improve parallelism in RoPE with pos_ids,The previous kernel was not parallelised sufficiently well for low batch sizes.,https://github.com/flashinfer-ai/flashinfer/pull/609
607,2024-11-13T03:53:17Z,test: add DtypeKV template param in bench_batch_decode,Add typename TKV so that it's convenient to benchmark FP8 KVCache.,https://github.com/flashinfer-ai/flashinfer/pull/607
606,2024-11-11T06:28:14Z,doc: improve the docstring of `append_paged_kv_cache`,Remove unnecessary note.,https://github.com/flashinfer-ai/flashinfer/pull/606
605,2024-11-11T06:16:10Z,feat: simplify prefill JIT compilation,Compile all three mask modes (causal/non-causal/custom) altogether instead of compiling them one-by-one.,https://github.com/flashinfer-ai/flashinfer/pull/605
602,2024-11-11T00:24:18Z,perf: fix prefill kernel performance degradation (step 1),"The prefill attention kernel performance has degraded significantly in recent releases (since v0.1.2), especially on A100 when causal=True,â€¦",https://github.com/flashinfer-ai/flashinfer/pull/602
601,2024-11-10T23:43:51Z,hotfix: fix rope tvm wrapper,"The TVM wrapper was broken in #599 because of API changes, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/601
599,2024-11-10T05:46:08Z,feat: add `rotary_dim` argument to rope APIs for partial apply rope,"This PR implements the final piece of #530 , so that we can partially apply rotary embedding to first head dimensions instead of entire heaâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/599
597,2024-11-08T16:30:12Z,Fix the type of `paged_kv_cache` in append,The type is adjusted to be consistent with the prefill/decode wrappers.,https://github.com/flashinfer-ai/flashinfer/pull/597
594,2024-11-08T06:01:43Z,misc: refactor cutlass includes,Move cutlass utilities outside of gemm folder as we are about to use cutlass for attention kernels.,https://github.com/flashinfer-ai/flashinfer/pull/594
590,2024-11-06T22:51:08Z,include convert latency in bench_append_paged_kv_cache,"model: l1b seqlens: [1, 1, 1, 1, 1, 1, 1, 1] convert: 45us 1layer: 7us 16layers: 151us throughput: 4.936GB/s model: l1b seqlens: [4993, 1,â€¦",https://github.com/flashinfer-ai/flashinfer/pull/590
588,2024-11-06T22:02:19Z,perf: fix the performance issue of `append_paged_kv_cache`,"The performance of append_paged_kv_cache is terrible for small batch size, which is a known issue that we haven't fixed for a long time, thâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/588
585,2024-11-05T11:44:16Z,feat: support cached cos/sin in rope APIs,"As requested in #530 , this PR implements the RoPE with cached cos/sin embeddings, which is more flexible in some use cases.",https://github.com/flashinfer-ai/flashinfer/pull/585
582,2024-11-05T10:39:44Z,fix broken tvm integration caused by #568,"Hi, now tvm wrapper build failed cause by #568.",https://github.com/flashinfer-ai/flashinfer/pull/582
583,2024-11-05T05:53:40Z,add benchmark for append_paged_kv_cache,Add a python benchmark for append_paged_kv_cache.,https://github.com/flashinfer-ai/flashinfer/pull/583
578,2024-11-02T02:40:48Z,return type overload for return_lse,Make mypy and pylance happy.,https://github.com/flashinfer-ai/flashinfer/pull/578
551,2024-11-02T02:39:25Z,feat: support MLA decode,"Hi, this PR implements MLA decode algorithm, I would love to hear your thoughts on this design and implementation.",https://github.com/flashinfer-ai/flashinfer/pull/551
573,2024-10-31T08:54:35Z,Fix Sphinx,"Here's the reason why docs fail to build after #552: As specified in conf.py, Sphinx mocks torch.",https://github.com/flashinfer-ai/flashinfer/pull/573
572,2024-10-30T12:54:36Z,fix broken cpp integration caused by #567,"Hi, cpp integration was broken again by #567, please be aware that there are cpp test, cpp benchmark and also tvm integration, they all relâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/572
567,2024-10-30T07:20:25Z,refactor: Refactor JIT and AOT build script,"Previously, JIT and AOT packaging is a bit broken.",https://github.com/flashinfer-ai/flashinfer/pull/567
569,2024-10-30T02:34:57Z,torch custom_op fix for rope,Fix after changes made in #568 torch.compile doesn't like returning input arguments.,https://github.com/flashinfer-ai/flashinfer/pull/569
568,2024-10-29T21:51:19Z,feat: support huggingface transformer style rope interface,"Previously our rope apis assume the position indices of each request is contiguous, which is not appropriate for applications such as specuâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/568
563,2024-10-27T09:38:09Z,bugfix: fix the sliding window iteration bound for SWA in batch prefill operators,"The iteration bound for sliding window in batch prefill kernels is wrong, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/563
561,2024-10-26T22:01:58Z,perf: remove unnecessary contiguous operation in block sparse attention,The contiguous operation is no longer required after #513,https://github.com/flashinfer-ai/flashinfer/pull/561
560,2024-10-26T21:37:10Z,perf: use cuda-core implemention for io-bound block-sparse attention,"When operational intensity is low, select cuda-core implementations for block-sparse attention.",https://github.com/flashinfer-ai/flashinfer/pull/560
559,2024-10-26T21:29:43Z,bugfix: fix `batch_prefill.cu` in AOT mode after #554,#554 didn't update the batch_prefill.cu (which was used in AOT mode) according to the API change.,https://github.com/flashinfer-ai/flashinfer/pull/559
554,2024-10-25T02:51:12Z,feat: torch.compile and custom_op support,Follow up of #552.,https://github.com/flashinfer-ai/flashinfer/pull/554
556,2024-10-25T02:39:14Z,bugfix: fix block sparse wrappers,"The block sparse attention unittests failed as noted in #554, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/556
553,2024-10-25T02:09:08Z,feat: non-contiguous query with paged kv cache,"Motivation Previously, only ragged version of prefill kernel supported non-contiguous query tensor (#404).",https://github.com/flashinfer-ai/flashinfer/pull/553
544,2024-10-22T08:28:03Z,bugfix: Fix the default value of `data_type` in batch decode plan function,"As mentioned in #543 , the behavior of batch decode plan function is problematic if we use the combo of q_data_type and kv_data_type.",https://github.com/flashinfer-ai/flashinfer/pull/544
546,2024-10-22T08:16:47Z,bugfix: fix jit compilation for single decode with tensor cores,The arguments have wrong order.,https://github.com/flashinfer-ai/flashinfer/pull/546
542,2024-10-20T08:27:09Z,bugfix: backward compatibility,"We recently changed the plan function signature and remove the data_type argument, which is not compatible with some old version.",https://github.com/flashinfer-ai/flashinfer/pull/542
540,2024-10-19T07:35:05Z,"Fix the ""not enough values to unpack"" error.","This PR fixes the following error: File ""/home/yuxianq/flashinfer/python/flashinfer/decode.py"", line 616, in plan self._cached_module = getâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/540
536,2024-10-18T00:19:42Z,bugfix: fix JIT compilation of prefill kernels,"Some bugs were introduced in #534, this PR fix these issues.",https://github.com/flashinfer-ai/flashinfer/pull/536
535,2024-10-17T09:58:27Z,jit: simplify jit example,"use generate_module function fix the correctness issue with flashsigmoid (when use_softmax is false, use attention sum instead of merge staâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/535
533,2024-10-16T06:18:04Z,feat: add a `use_softmax` field in variant class,"If true, use online softmax rules to update attention states.",https://github.com/flashinfer-ai/flashinfer/pull/533
527,2024-10-11T20:31:18Z,bugfix: fix the stride bug in page append,"We introduced a bug in #513 because we didn't consider non-contiguous kv-cache for page append operator, this PR fix the bug.",https://github.com/flashinfer-ai/flashinfer/pull/527
525,2024-10-11T06:36:49Z,misc: remove unused functions in prefill parameters,get_mask_offset was deprecated.,https://github.com/flashinfer-ai/flashinfer/pull/525
518,2024-10-09T23:36:39Z,refactor: remove warp layout enum,"In our previous design of WarpLayout abstractions, we mix (1) the warp layout (number of warps on q and kv) and (2) the number of fragmentsâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/518
513,2024-10-09T11:00:43Z,Feature/non contiguous kv cache,This PR solves #506 Custom strides to support non-contiguous kv cache.,https://github.com/flashinfer-ai/flashinfer/pull/513
507,2024-10-07T00:17:54Z,feat: JIT compilation,"This PR implements the JIT compilation (#170 ) of flashinfer, after this PR, flashinfer will compile kernels just-in-time for different inpâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/507
505,2024-09-25T08:25:02Z,fix: batch decode kernel redundant store output to gmem,"Hi, this is a minor fix, when bdz is greater than 1, there would be redundant store to gmem operations for some warps.",https://github.com/flashinfer-ai/flashinfer/pull/505
473,2024-08-27T10:05:15Z,hotfix: skip prefill kernels on sm75 for bf16,Followup of #472 .,https://github.com/flashinfer-ai/flashinfer/pull/473
447,2024-08-27T01:18:37Z,chore(main): release 0.1.6,"ðŸ¤– I have created a release beep boop 0.1.6 (2024-08-27) SM75 Support Starting from 0.1.6, our pre-built wheels include experimental supportâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/447
466,2024-08-25T08:56:21Z,refactor: replace `begin_forward`/`forward`/`end_forward` with `plan`/`run`,This PR changes the use of begin_forward/forward/end_forward API with the new plan/run API.,https://github.com/flashinfer-ai/flashinfer/pull/466
463,2024-08-23T03:54:09Z,doc: another bunch of documentation improvement,fix broken links add rope docs add default option for sampling apis,https://github.com/flashinfer-ai/flashinfer/pull/463
462,2024-08-22T11:28:03Z,feat: add `MultiLevelCascadeAttentionWrapper` API,Our existing cascade inference APIs all assumes shared prefix kv-cache are standalone tensors which is not the case for real-world llm servâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/462
459,2024-08-21T19:08:36Z,perf: use persistent kernel for merging attention states,"As observed by @MasterJH5574 , there are cases where our VariableLengthMergeStatesKernel launches a lot of CTAs (>=10k) while most of the Câ€¦",https://github.com/flashinfer-ai/flashinfer/pull/459
460,2024-08-21T17:21:46Z,bugfix: fix the python api of prefill wrapper + custom mask,"some tests are failed because we use the wrong function for prefill wrapper with custom mask, this pr fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/460
448,2024-08-16T00:55:45Z,bugfix: fix the prefill/append attention kernel accuracy issue on sm75,"As reported by @esmeetu , the prefill/append attention kernel produce incorrect results on sm75.",https://github.com/flashinfer-ai/flashinfer/pull/448
446,2024-08-14T09:47:11Z,fix: resolve cu121 compile wired issue,cc @yzh119 @Ying1123 @Yard1 @comaniac,https://github.com/flashinfer-ai/flashinfer/pull/446
435,2024-08-13T10:19:31Z,chore(main): release 0.1.5,ðŸ¤– I have created a release beep boop 0.1.5 (2024-08-13) Bugfix Fix PagedPrefill python api and some typos (#441) (3fff008) fix prefill kernâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/435
442,2024-08-13T10:02:38Z,feat: decouple float and int workspace buffer,"Before this PR, flashinfer coupled float and int buffers in a single workspace buffer, and different wrappers cannot share the same buffers.",https://github.com/flashinfer-ai/flashinfer/pull/442
441,2024-08-13T09:26:56Z,Fix PagedPrefill python api and some typos,"Fix two small bugs: â€œNHDâ€ and ""HND"" used confusing PagedPrefill use self._custom_mask_buf to judge whether is customized_mask, but uninitiaâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/441
440,2024-08-13T07:54:27Z,bugfix: fix prefill kernels' lse result for empty kv-cache,Thank @hnyls2002 for spotting this bug.,https://github.com/flashinfer-ai/flashinfer/pull/440
415,2024-08-09T09:07:01Z,chore(main): release 0.1.4,ðŸ¤– I have created a release beep boop 0.1.4 (2024-08-09) Features append attention kernels for fp8 kv-cache (#420) (906c2f5) support min_p sâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/415
426,2024-08-07T08:11:27Z,docs: update README,fix cc @yzh119 /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/crti.o: in function `_init': (.init+0xb): relocation truncated toâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/426
420,2024-08-06T09:26:03Z,feat: append attention kernels for fp8 kv-cache,"This implementation do not rely on fp8 tensor cores, but uses fp16 tensor cores instead (so sm_80 architectures can also use it), the fp8 kâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/420
414,2024-07-31T10:47:28Z,bump version: v0.1.3,0.1.3 (2024-07-31) Bugfix bugfix: Fix cudagraph mode of BatchPrefillWithRaggedKVCacheWrapper (#412) (9907bc) fix cu118 cub usage for sampliâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/414
413,2024-07-31T10:38:34Z,misc: enhance allocator error info and add shape check for prefill begin forward functions,"This PR makes the following changes to the codebase: make the allocators error information more informative, more specifically, we print thâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/413
407,2024-07-29T12:26:37Z,ptx: fragment layout swizzling kernels,"Composed instructions for fragment layout swizzling, used in our fp8 (fake/real quantization) prefill kernels.",https://github.com/flashinfer-ai/flashinfer/pull/407
394,2024-07-29T12:11:13Z,chore(main): release 0.1.2,"ðŸ¤– I have created a release beep boop 0.1.2 (2024-07-29) Bugfix Fix the sampling kernel bug for cu118 (#386, #387) (0cd499, dc3f18) Featuresâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/394
406,2024-07-29T12:05:33Z,feat: sliding window attention,"As requested in #390 , this PR implements sliding window attention.",https://github.com/flashinfer-ai/flashinfer/pull/406
405,2024-07-29T04:39:24Z,feat: non-inplace rope operators,"As requested in #403, this PR implements non-inplace rope operators.",https://github.com/flashinfer-ai/flashinfer/pull/405
404,2024-07-29T03:18:19Z,feat: support non-contiguous (packed) input for prefill kernels,"This PR implements #311 , after this PR, we support packed qkv input without explictly convert make the input contiguous: packed_qkv = W_qkâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/404
401,2024-07-27T10:37:28Z,feat: add llama 3.1 style rope,Reference implementation: https://github.com/meta-llama/llama-models/blob/709a61fd810157f75fbb314e7287089eec06d9c3/models/llama3_1/api/modeâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/401
388,2024-07-21T04:28:14Z,misc: clang-format prefill.cuh,misc: clang-format prefill.cuh,https://github.com/flashinfer-ai/flashinfer/pull/388
381,2024-07-20T09:15:22Z,chore(main): release 0.1.1,ðŸ¤– I have created a release beep boop 0.1.1 (2024-07-20) Bugfix fix the invalid kernel configuration for architectures with small shared memâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/381
383,2024-07-20T01:24:59Z,feat: expose decoupled kv-cache to pytorch api,Followup of #379,https://github.com/flashinfer-ai/flashinfer/pull/383
379,2024-07-18T08:38:36Z,refactor: decouple kv-cache storage,"In our previous design, k-cache and v-cache are coupled together as a (num_pages, 2, page_size, num_heads, head_dim) or a (num_pages, 2, nuâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/379
373,2024-07-17T08:29:28Z,chore(main): release 0.1.0,ðŸ¤– I have created a release beep boop 0.1.0 (2024-07-17) Features Add mask to merge_state_in_place (#372) (e14fa81) expose pytorch api for bâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/373
375,2024-07-17T08:28:40Z,feat: expose pytorch api for block sparse attention,"The block sparse attention (for any block size (R, C)) are hidden in flashinfer's codebase but it was never exposed explicitly in python.",https://github.com/flashinfer-ai/flashinfer/pull/375
359,2024-07-12T05:54:32Z,chore(main): release 0.0.9,ðŸ¤– I have created a release beep boop 0.0.9 (2024-07-12) Bugfix fix the decode kernel segfault in cudagraph mode (#368)(c69cfa) fix decode kâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/359
368,2024-07-11T06:16:48Z,bugfix: fix the decode kernel segfault in cudagraph mode,"The begin_forward function in decode attention wrappers sometimes triggers segfault, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/368
364,2024-07-10T08:22:30Z,refactor: slight refactor of prefill kernels,add __launch_bounds__ add unroll hint for prefetching page indices change loop structure of threadblock_sync_mdo_states,https://github.com/flashinfer-ai/flashinfer/pull/364
363,2024-07-10T07:01:39Z,bugfix: fix decode kernels output for empty kv cache,"When some request has empty kv cache, the output of decode kernels doesn't align with prefill kernels.",https://github.com/flashinfer-ai/flashinfer/pull/363
356,2024-07-04T07:57:33Z,perf: accelerate gqa performance,"Changes: Prefetch page indices (we have already done such optimization on decode kernels, but not on append/prefill kernels which was usedâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/356
355,2024-07-03T07:57:31Z,bump version: v0.0.8,0.0.8 (2024-07-03) Bugfix fix prefill/append kernel behavior for empty kv-cache (#353) (7adc8c) fix decode attention kernel with logits capâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/355
353,2024-07-03T07:42:37Z,bugfix: fix prefill/append kernel behavior for empty kv-cache.,"The prefill kernels was buggy when some of the requests have empty kv-cache, this PR fixes the issue.",https://github.com/flashinfer-ai/flashinfer/pull/353
352,2024-07-03T04:47:24Z,tests: add more unittests for logits cap,followup of #350 add the case of logits_soft_case=1.0 to correctness tests.,https://github.com/flashinfer-ai/flashinfer/pull/352
350,2024-07-03T03:32:19Z,hotfix: fix the decode kernel with logits cap,logits soft cap should be applied before masking.,https://github.com/flashinfer-ai/flashinfer/pull/350
343,2024-06-30T06:58:57Z,refactor: reduce the binary size of batch decode kernels,"This PR refactors the batch decode related kernels, and make the following breaking changes: remove batch_decode_with_padded_kv_cache operaâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/343
340,2024-06-28T17:48:16Z,[CMake][Bugfix] Set default value for FLASHINFER_GEN_MASK_MODES,This commit resolves a build-time error with the following message: CMake Error at 3rdparty/flashinfer/CMakeLists.txt:313 (add_library): Noâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/340
338,2024-06-28T07:40:13Z,benchmark: add batch prefill with ragged kv-cache benchmark,benchmark: add batch prefill with ragged kv-cache benchmark,https://github.com/flashinfer-ai/flashinfer/pull/338
232,2024-06-20T08:41:56Z,chore(main): release 0.0.5,ðŸ¤– I have created a release beep boop 0.1.0 (2024-06-20) Highlights Support any GQA group size support for tensor-cores kernels.,https://github.com/flashinfer-ai/flashinfer/pull/232
317,2024-06-20T08:14:04Z,feat: add `use_tensor_cores` option to decode kernels to accelerate GQA,The tensor-cores accelerated GQA in our blog post was not enabled by default (user need to use Prefill kernels/wrappers for decode to get sâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/317
310,2024-06-20T06:47:07Z,perf: split kv-cache for prefill/append kernels,"Duplicate of #75, but re-based on the main branch.",https://github.com/flashinfer-ai/flashinfer/pull/310
308,2024-06-16T07:50:50Z,perf: use packed bit array for attention mask,float attention mask consumes too much gpu memory and makes the attention kernel slow.,https://github.com/flashinfer-ai/flashinfer/pull/308
306,2024-06-15T20:30:26Z,refactor: remove `page_size` from template parameters for prefill kernels,"Similar to #301 , in this PR we remove page_size from template parameters so that we can support any page_size for prefill kernels (previouâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/306
301,2024-06-15T06:44:05Z,rafactor: move `gqa_group_size` from template parameter to input arguments,"#262 is out of sync with main, this PR rebased the code on main branch.",https://github.com/flashinfer-ai/flashinfer/pull/301
300,2024-06-14T09:39:47Z,doc: fix logits cap docstring,"follow up of #299, pre-attention -> pre-softmax.",https://github.com/flashinfer-ai/flashinfer/pull/300
299,2024-06-14T09:36:44Z,doc: fix the description of logits cap in docstring,"The logits cap was applied to pre-attention logits, not attention scores.",https://github.com/flashinfer-ai/flashinfer/pull/299
286,2024-06-13T23:47:20Z,feat: Separate Q and KV dtypes for decode,Closes #285 Modified unit tests pass.,https://github.com/flashinfer-ai/flashinfer/pull/286
294,2024-06-10T10:33:26Z,refactor: refactor decode handler,Change the use of an optional fixed_grid_size to padded_batch_size.,https://github.com/flashinfer-ai/flashinfer/pull/294
291,2024-06-10T07:07:29Z,bugfix: Fix the behavior of decode cuda graph wrapper,"In this PR, we fixed the grid size of the kernels launched by a decode cuda graph wrapper: We always pad the block size to a fixed value.",https://github.com/flashinfer-ai/flashinfer/pull/291
289,2024-06-08T08:24:26Z,feat: initial support of distributed operators,This PR implements the attention all-reduce kernel which will be used in merging attention states from different GPUs in sequence paralleliâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/289
287,2024-06-07T08:13:26Z,cmake: fix DECODE_F8_DTYPES and DECODE_FP8_DTYPES discrepancy,"As a result of typo, it appeared two variables: DECODE_F8_DTYPES and DECODE_FP8_DTYPES in CMakeLists.txt.",https://github.com/flashinfer-ai/flashinfer/pull/287
281,2024-06-04T05:21:43Z,bugfix: fix cudagraph-compatible prefill/decode apis,The indptr array length should be a upper-bound of batch_size + 1 in cuda graph mode.,https://github.com/flashinfer-ai/flashinfer/pull/281
277,2024-06-02T09:14:42Z,feat: support cuda graph for batched multi-query(prefill/append) attention,"#275 is not complete, this pr pushes the remaining changes.",https://github.com/flashinfer-ai/flashinfer/pull/277
276,2024-06-02T09:11:47Z,"Revert ""feat: support cuda graph for batched multi-query(prefill/append) attention""",Reverts #275,https://github.com/flashinfer-ai/flashinfer/pull/276
275,2024-06-02T09:08:11Z,feat: support cuda graph for batched multi-query(prefill/append) attention,Followup of #187 and #256,https://github.com/flashinfer-ai/flashinfer/pull/275
273,2024-06-01T10:35:28Z,fp8: add calibration scale for decode attention operators,@comaniac,https://github.com/flashinfer-ai/flashinfer/pull/273
271,2024-05-30T01:08:24Z,doc: add some documentation for attention with mask API,add fix the bugs in single_prefill_with_kv_cache and single_prefill_with_kv_cache_return_lse when using mask.,https://github.com/flashinfer-ai/flashinfer/pull/271
266,2024-05-28T06:24:29Z,feat: support custom attention mask in prefill/append attention kernels,"Some speculative decoding algorithms requires tree attention, which could be supported via prefill/append attention kernels with custom attâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/266
256,2024-05-24T22:36:03Z,perf: initial cuda graph support,"As requested in #187 , this PR adds initial support of CUDAGraph compatibility of flashinfer batch decode attention kernels.",https://github.com/flashinfer-ai/flashinfer/pull/256
223,2024-05-15T06:03:22Z,support versatile gqa size for batch prefill,This merge request supports versatile gqa size for batch prefill kernels.,https://github.com/flashinfer-ai/flashinfer/pull/223
209,2024-04-24T21:44:34Z,move dispatch for batch prefill,move DISPATCH_PAGE_SIZE out of BatchPrefillWithPagedKVCacheWrapperDispatched,https://github.com/flashinfer-ai/flashinfer/pull/209
203,2024-04-16T21:09:59Z,Update CMakeLists.txt,Update COMMAND python to COMMAND python3 to ensure that build uses python 3.X rather than defaulting to python 2.X in case python is not inâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/203
198,2024-04-09T00:47:25Z,[CMake] Add positional independent code (PIC) option to kernels,This PR adds the -fPIC option to prefill/decode kernels for packaging.,https://github.com/flashinfer-ai/flashinfer/pull/198
177,2024-03-12T09:13:05Z,fix: fatal bugfix in batch decode operator,The BatchDecodeWithPagedKVCacheWrapper didn't run into the kernel.,https://github.com/flashinfer-ai/flashinfer/pull/177
173,2024-03-11T10:28:40Z,bugfix: Fix release wheel script and remove uninstantiated branches in dispatch,The release action failed because action-gh-release action do not support uploading multiple large files at a time: softprops/action-gh-relâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/173
172,2024-03-11T04:24:05Z,ci: reduce binary size,Do not generate prefill kernels for page_size=8 Build with -Xfatbin=-compress-all to reduce binary size.,https://github.com/flashinfer-ai/flashinfer/pull/172
171,2024-03-11T04:14:12Z,1. Reduce compile time by 22%+ 2. Fix compile linking error on Ubuntu 22.04 gcc/g++ 11.4 with Cuda 12.4,Test Env: Os/Container: Ubuntu 22.04 GCC/G++: 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) Cpu: Zen3 9334 (2x) Cpu cores/threads: Container limiteâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/171
167,2024-03-11T01:47:26Z,refactor: decouple attention implementation and declaration in different header files,To accelerate compilation.,https://github.com/flashinfer-ai/flashinfer/pull/167
120,2024-03-08T10:05:53Z,chore(main): release 0.0.3,ðŸ¤– I have created a release beep boop 0.0.3 (2024-03-08) Features adding sm_scale field for all attention APIs (#145) (85d4018) enable head_â€¦,https://github.com/flashinfer-ai/flashinfer/pull/120
156,2024-03-05T12:00:58Z,feat: pytorch api of fp8 kv-cache,requested in #150 #155 #125,https://github.com/flashinfer-ai/flashinfer/pull/156
146,2024-03-03T11:23:43Z,feat: support ALiBi,Implement attention with linear bias (ALiBi).,https://github.com/flashinfer-ai/flashinfer/pull/146
145,2024-03-01T11:51:50Z,feat: adding `sm_scale` field for all attention APIs,"Some of our attention APIs have this field and some don't, this PR add sm_scale field for all attention APIs to make them consistent.",https://github.com/flashinfer-ai/flashinfer/pull/145
144,2024-03-01T00:07:43Z,perf: multiple q by sm_scale in decode kernels,"The same optimization was used in our prefill attention kernels, this PR applies this optimization to decode attention kernels.",https://github.com/flashinfer-ai/flashinfer/pull/144
143,2024-02-29T09:50:55Z,refactor: move attention related headers to flashinfer/attention,"Refactor code structure, we will have not only attention kernels, but also lora/dequantize/sampling kernels soon and they will be placed unâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/143
132,2024-02-25T02:12:51Z,feat: enable `head_dim=256` for attention kernels,"As mentioned in #130 , the kernels for head_dim=256 are not compiled by default, this PR expose these attention kernels to pip wheels and aâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/132
128,2024-02-23T07:36:09Z,support Turing arch,"As I want to push this feature forward, so i tried to work based on #109 , and did some small changes.",https://github.com/flashinfer-ai/flashinfer/pull/128
126,2024-02-19T16:18:59Z,Passing in attn_score_scaling_factor into tvm_wrapper,"In GPT-2, attention calculation requires an additional feature scale_attn_by_inverse_layer_idx.",https://github.com/flashinfer-ai/flashinfer/pull/126
69,2024-02-01T22:27:04Z,Support RoPE position info in batch prefill/decode kernels,This PR adds q/k position information to batch prefill/decode kernels.,https://github.com/flashinfer-ai/flashinfer/pull/69
88,2024-01-26T01:51:06Z,[Performance] Using user-allocated workspace for batch decode/prefill handlers,To avoid the overhead of allocating/destroy memory per step.,https://github.com/flashinfer-ai/flashinfer/pull/88
85,2024-01-24T13:04:59Z,[Refactor] Formalize NHD/HND layout annotation,"We support two different layout annotations (NHD and HND, N: sequence lenght, H: number of heads, D: head dimension) for QKV matrices.",https://github.com/flashinfer-ai/flashinfer/pull/85
84,2024-01-21T10:13:40Z,[Performance] Fix the fp8 decode kernel performance degradation issue,The fp8 decode kernel performance degrades because of the changes in previous commits 2a3d6d0 .,https://github.com/flashinfer-ai/flashinfer/pull/84
72,2024-01-18T11:07:49Z,[Refactor] Use two kernels instead of CUDA cooperative kernel for batch/single decode,In our initial design we use CUDA cooperative kernels and grid synchronization feature for cross threadblock reduction.,https://github.com/flashinfer-ai/flashinfer/pull/72
68,2024-01-10T15:02:51Z,[Performance] Another prefill/append parameter tweak,[Performance] Another prefill/append parameter tweak,https://github.com/flashinfer-ai/flashinfer/pull/68
61,2024-01-08T14:32:10Z,[Refactor] Update the BeginForward API for batch prefill handler,gqa_group_size is less intuitive than num_qo_heads + num_kv_heads.,https://github.com/flashinfer-ai/flashinfer/pull/61
58,2024-01-08T09:05:11Z,[Bugfix] Fix the lse computation for batch decode kernel,The lse returned by BatchDecodeWithPagedKVCacheKernel is wrong if Paged-KV is splitted.,https://github.com/flashinfer-ai/flashinfer/pull/58
55,2024-01-07T09:02:11Z,[Performance] Tweak Prefill Parameters,[Performance] Tweak Prefill Parameters,https://github.com/flashinfer-ai/flashinfer/pull/55
48,2024-01-03T15:48:58Z,[Performance] Accelerate append prefill kernel,We found that cooperative kernel (merge states in the same kernel afte grid synchronization) actually limits kernel's performance when mergâ€¦,https://github.com/flashinfer-ai/flashinfer/pull/48
41,2024-01-01T10:02:30Z,[Performance] Cooperative prefill parameter tweak,Note that prefill kernel's synchronization overhead is larger than decode's.,https://github.com/flashinfer-ai/flashinfer/pull/41
36,2024-01-01T05:09:19Z,"Tweak cooperative prefill parameters, and increase maximum context length in benchmark to 65536","Tweak cooperative prefill parameters, and increase maximum context length in benchmark to 65536",https://github.com/flashinfer-ai/flashinfer/pull/36
33,2023-12-30T17:13:21Z,Follow-up split-kv batch decode fix,Follow-up split-kv batch decode fix,https://github.com/flashinfer-ai/flashinfer/pull/33
30,2023-12-25T05:14:07Z,batch_prefill python api,"A python api PR,",https://github.com/flashinfer-ai/flashinfer/pull/30
26,2023-12-20T04:04:26Z,Fix Rotary Embedding kernel,Fix Rotary Embedding kernel,https://github.com/flashinfer-ai/flashinfer/pull/26
20,2023-12-06T06:44:17Z,add tvm wrapper for prefill,add tvm wrapper for prefill,https://github.com/flashinfer-ai/flashinfer/pull/20
18,2023-11-27T06:07:43Z,allow compiling batch_prefill and batch_decode in separate files,allow compiling batch_prefill and batch_decode in separate files,https://github.com/flashinfer-ai/flashinfer/pull/18
17,2023-11-27T00:22:40Z,PyTorch API for Single Request Prefill Operator,add prefill kernel pytorch api,https://github.com/flashinfer-ai/flashinfer/pull/17
13,2023-11-11T03:44:37Z,[Decode] KV cache split pre-process update,"This PR applies the following changes to the KV cache split: removed the map structure for temporary array cache, changed cudaMallocAsync tâ€¦",https://github.com/flashinfer-ai/flashinfer/pull/13
12,2023-11-08T18:38:52Z,[Wrapper] Split prefill/decode wrapper,This PR splits the TVM wrapper so that prefill and decode has their own wrapper function.,https://github.com/flashinfer-ai/flashinfer/pull/12
10,2023-10-31T23:20:03Z,Skipping prefill for empty sequence,Skipping prefill for empty sequence,https://github.com/flashinfer-ai/flashinfer/pull/10
7,2023-10-15T02:21:22Z,[Fix] Free allocated CUDA memory in prefill,[Fix] Free allocated CUDA memory in prefill,https://github.com/flashinfer-ai/flashinfer/pull/7
5,2023-10-11T02:59:02Z,Wrapper support for prefill with KV cache,Wrapper support for prefill with KV cache,https://github.com/flashinfer-ai/flashinfer/pull/5
4,2023-10-09T04:17:48Z,[Wrapper] Add the CSR indptr of append lengths to interface,This PR also unifies the decode wrapper with the (potential) prefill wrapper.,https://github.com/flashinfer-ai/flashinfer/pull/4
3,2023-09-18T05:16:00Z,bench batch decode,bench batch decode,https://github.com/flashinfer-ai/flashinfer/pull/3
2,2023-09-13T18:07:49Z,Add tmp buffer and rotary mode to BatchDecode wrapper,Add tmp buffer and rotary mode to BatchDecode wrapper,https://github.com/flashinfer-ai/flashinfer/pull/2
1,2023-09-11T04:19:18Z,[Wrapper] TVM wrapper for batch-decode kernel without RoPE,Will extend with RoPE later on.,https://github.com/flashinfer-ai/flashinfer/pull/1
